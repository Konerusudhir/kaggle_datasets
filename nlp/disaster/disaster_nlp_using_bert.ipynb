{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this kernel\n\n[Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/4) is a model created and publicly made available by Google. Built in Tensorflow, it was trained to embed any type of sentences or short paragraphs so that the meaning is as much preserved as possible; so that it can be finetuned for classification tasks specifically.\n\nThis implementation is extremely pleasant to use, since the input is simply the string, and the output is just the 512-dimensional encoded sentence; no preprocessing is needed. It is also fully-trainable, and uses an almost state-of-the-art architecture (namely, pre-BERT transformers (nice [comparison in this blog post](https://blog.floydhub.com/when-the-best-nlp-model-is-not-the-best-choice/)).\n\nSince this competition is dedicated for beginners to get started, I feel this is a perfect example of using a novel technology, but packaged in a gentle and correctly abstracted API (as opposed to the horrors of [1000 lines of tensorflow code](https://github.com/google-research/bert/blob/master/modeling.py) that needs to be understood before modifying BERT). Here, instead, **I'm only showing you some 50 lines of codes to get all up and running**; and only ~15 lines to setup the model!\n\n## Summary\n\nThis kernel serves as a short and straightforward introduction to the process of:\n1. Loading a trained model from [Tensorflow hub](https://tfhub.dev/).\n2. Building a `Sequential` Keras model by using the trained model as a layer.\n3. Training the newly created Keras model, and perform inference."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Tensorflow\n# !pip uninstall -y tf-hub-nightly\n# !pip uninstall -y tensorflow-hub\n# !pip uninstall -y tensorflow\n# !pip install tensorflow==2.0.0\n# !pip install tensorflow_hub==0.7.0\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n# from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, BatchNormalization, Dropout, Concatenate, Layer\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras import regularizers\n\n!pip install bert-for-tf2\nimport bert\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import average_precision_score, auc, classification_report, confusion_matrix, roc_curve, precision_recall_curve\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport nltk\nfrom collections import defaultdict\nfrom collections import  Counter\nimport re\nimport gensim\nimport string\nfrom tqdm import tqdm, tqdm_notebook\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For finding Tensorflow version\nprint(\"TF version: \", tf.__version__)\nprint(\"Hub version: \", hub.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data and model"},{"metadata":{},"cell_type":"markdown","source":"First load all the CSV files we will need"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\nprint(type(train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create convenient names for the variables we will be using for training and inference."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data = train.text.values\n# train_labels = train.target.values\n# test_data = test.text.values\n# train_data.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clean Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install contractions\n!pip install beautifulsoup4\n\nimport contractions\nfrom bs4 import BeautifulSoup\nimport unicodedata\nimport re\n\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    [s.extract() for s in soup(['iframe', 'script'])]\n    stripped_text = soup.get_text()\n    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n    return stripped_text\n\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\ndef expand_contractions(text):\n    return contractions.fix(text)\n\ndef remove_special_characters(text, remove_digits=False):\n    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n    text = re.sub(pattern, '', text)\n    return text\n\ndef pre_process_document(document):\n    # strip HTML\n    document = strip_html_tags(document)\n    # lower case\n    document = document.lower()\n    # remove extra newlines (often might be present in really noisy text)\n    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n    # remove accented characters\n    document = remove_accented_chars(document)\n    # expand contractions    \n    document = expand_contractions(document)  \n    # remove special characters and\\or digits    \n    # insert spaces between special characters to isolate them    \n    special_char_pattern = re.compile(r'([{.(-)!}])')\n    document = special_char_pattern.sub(\" \\\\1 \", document)\n    document = remove_special_characters(document, remove_digits=True)  \n    # remove extra whitespace\n    document = re.sub(' +', ' ', document)\n    document = document.strip()\n    \n    return document\n\n\npre_process_corpus = np.vectorize(pre_process_document)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n# pd.options.display.max_colwidth = 200\n\n# print(train[train['target']==1]['text'].values[:10])\n\n# print(train[train['target']==0]['text'].values[:10])\n\n# train_data = pre_process_corpus(train.text)\n# print(\"T Shape:\" + str(train_data.shape))\n# # train_data = pd.DataFrame(train_data)\n# train_labels = train.target.values\n# test_data = pre_process_corpus(test.text)\n\n# X_train,X_test,y_train,y_test=train_test_split(train_data,train_labels,test_size=0.20, random_state = 777, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load BERT Module"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nbert_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\nmax_seq_length = 256\n# bert_embedding_module = hub.Module(bert_path, trainable=False, name='bert_embedding_module_2')\n\nbert_embedding_layer = hub.KerasLayer(bert_path, trainable=False, name='bert_embedding_layer')\n# hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n#                             trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\n\ndef get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\n\ndef get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\n# vocab_file = bert_embedding_layer.resolved_object.vocab_file.asset_path.numpy()\n# do_lower_case = bert_embedding_layer.resolved_object.do_lower_case.numpy()\nFullTokenizer = bert.bert_tokenization.FullTokenizer\n# tokenizer = FullTokenizer(vocab_file, do_lower_case)\n\nvocab_file = bert_embedding_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_embedding_layer.resolved_object.do_lower_case.numpy()\ntokenizer = FullTokenizer(vocab_file, do_lower_case)\n\n# tokenizer.tokenize(\"sadas asdasd   asdasd\").shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = [1,3]\nprint(type(a))\n# print(a.shape)\n\nnp.array(a, dtype=object)[:, np.newaxis]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_sentences_to_features(tokenizer, sentences, max_seq_length=256, is_test = False):\n    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n    \n    stokens = sentences[\"text\"].apply(lambda x : [\"[CLS]\"] + tokenizer.tokenize(x) + [\"[SEP]\"] )\n#     np.array(a, dtype=object)[:, np.newaxis]\n    stokens_ids = np.matrix(stokens.apply(lambda x : get_ids(x, tokenizer, max_seq_length)).tolist())\n    stokens_masks = np.matrix(stokens.apply(lambda x : get_masks(x, max_seq_length)).tolist())\n    stokens_segments = np.matrix(stokens.apply(lambda x : get_segments(x, max_seq_length)).tolist())\n\n    assert stokens_ids[0].size == max_seq_length\n    assert stokens_masks[0].size == max_seq_length\n    assert stokens_segments[0].size == max_seq_length\n\n    if is_test:\n        return (\n            stokens_ids,\n            stokens_masks,\n            stokens_segments,\n        )\n    else:\n        return (\n            stokens_ids,\n            stokens_masks,\n            stokens_segments,\n            np.array(sentences[\"target\"]).reshape(-1,1),\n        )\n\n# print(train)\n(all_input_ids, all_input_masks, all_segment_ids, all_labels \n) = convert_sentences_to_features(tokenizer, train, max_seq_length=max_seq_length)\n(test_input_ids, test_input_masks, test_segment_ids\n) = convert_sentences_to_features(tokenizer, test, max_seq_length=max_seq_length, is_test = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(all_input_ids))\n\n# print(all_input_ids.values[0:2])\n\nprint(all_input_ids.shape)\nprint(all_input_masks.shape)\nprint(all_segment_ids.shape)\nprint(all_labels.shape)\nprint(test_input_ids.shape)\nprint(test_input_masks.shape)\nprint(test_segment_ids.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LOAD BERT MODEL"},{"metadata":{},"cell_type":"markdown","source":"Finally, load the BERT Encoder from tfhub.dev (make sure Internet is enabled!)."},{"metadata":{},"cell_type":"markdown","source":"### Customize BERT Model"},{"metadata":{},"cell_type":"markdown","source":"# Train the model"},{"metadata":{},"cell_type":"markdown","source":"Build a simple sequential model in Keras, with just a few lines. Note that the `Input` here is a tf.string; usually you will see integer inputs followed by an `Embedding` layer; those are needed for RNNs or CNNs, but here it is all taken care of internally by the USE; in other words, the `embed` layer you just loaded is internally tokenizing the strings, convert them to integers, then map them using an embedding.\n\nIf none of those words make any sense, worry not! USE was designed to be easily understood and directly used as is, so you don't have to get into the low-level implementation details, and can focus on using it as a tool in your Keras model, or use it as is."},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.device('/gpu:0'):\n    def build_model(bert_embedding_layer, max_seq_length):\n#         input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n#                                        name=\"input_word_ids\")\n#         input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n#                                            name=\"input_mask\")\n#         segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n#                                             name=\"segment_ids\")\n        in_id = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_ids\")\n        in_mask = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_masks\")\n        in_segment = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n        bert_inputs = [in_id, in_mask, in_segment]\n        \n#         bert_output = BertLayer(n_fine_tune_layers=3, pooling=\"first\")(bert_inputs)\n        bert_pooled_output, bert_sequenced_output = bert_embedding_layer(bert_inputs)\n        x = Dense(10, activation='relu')(bert_pooled_output) \n#         x = Dense(64, activation='relu')(bert_pooled_output) \n#         x = Dense(32, activation='relu')(x)\n        output = Dense(1, activation='sigmoid')(x)\n        \n        model = Model(inputs=bert_inputs, outputs=output)\n        \n        optimizer = Adam(lr = 0.0001)\n\n        metrics = [\n            'accuracy', \n            tf.keras.metrics.Recall(),\n            tf.keras.metrics.Precision()\n        ]\n        model.compile(optimizer, loss='binary_crossentropy', metrics=metrics)\n\n        return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check if the model looks the way we want:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model( bert_embedding_layer, max_seq_length = max_seq_length)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's get started with the training step! We'll use 20% of the data to validate the results, and only save the model that has the lowest loss on that 20% data."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Shape of all inputs \",all_input_ids.shape)\nprint(\"Shape of all labels \",all_labels.shape)\n\n# print(\"Shape of all inputs \",np.array(all_input_ids).reshape(all_input_ids.shape[0],).shape)\n\n# train_input_ids, train_input_masks, train_segment_ids, train_labels \ntrain_input_ids, dev_input_ids, train_input_masks, dev_input_masks, train_segment_ids, dev_segment_ids, train_labels, dev_labels =train_test_split(\n    np.array(all_input_ids),\n    np.array(all_input_masks),\n    np.array(all_segment_ids),\n    np.array(all_labels),\n    test_size=0.20, \n    random_state = 777, \n    shuffle = True)\n\nprint(\"Shape of Train \",train_input_ids.shape)\nprint(\"Shape of Dev \",dev_input_ids.shape)\n\ncheckpoint = ModelCheckpoint('model_with_low_val_loss.h5', monitor='val_loss', mode='min', modelsave_best_only=True)\nearly_stopping = EarlyStopping(monitor='val_loss', mode='min', min_delta=0.001, patience=10)\ncallbacks = [early_stopping, checkpoint]\n\ntrain_history = model.fit(\n    [train_input_ids, train_input_masks, train_segment_ids], \n    train_labels,\n    validation_data=(\n        [dev_input_ids, dev_input_masks, dev_segment_ids],\n        dev_labels\n    ),\n    epochs=50,\n    callbacks=callbacks,\n    batch_size=32\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_loss = pd.DataFrame(model.history.history)\n# model_loss.head()\nmodel_loss[['loss','val_loss']].plot(ylim=[0,1])\nmodel_loss[['accuracy','val_accuracy']].plot(ylim=[0,1])\npd.DataFrame(model.history.history).filter(regex=\"precision\", axis=1).plot(ylim=[0,1])\npd.DataFrame(model.history.history).filter(regex=\"recall\", axis=1).plot(ylim=[0,1])\n# predictions = model.predict_classes(X_test) \n# print(classification_report(y_test, predictions, target_names=[\"Real\", \"Not Real\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Best Model"},{"metadata":{},"cell_type":"markdown","source":"## Calculate F1 Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"def findClasses(predictions):\n    true_preds = []\n    a=1\n    b=0\n\n    for i in predictions:\n        if i >= 0.5:\n            true_preds.append(a)\n        else:\n            true_preds.append(b)\n    return true_preds\nmodel.load_weights('model_with_low_val_loss.h5')\npredictions = model.predict([dev_input_ids, dev_input_masks, dev_segment_ids]) \nclasses = findClasses(predictions)\nprint(classification_report(dev_labels, classes, target_names=[\"Real\", \"Not Real\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{},"cell_type":"markdown","source":"Don't forget that the latest model might not be the best! Instead, the best is the one we saved as `model.h5`; let's load it and run prediction on `test_data`."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_pred = model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we round the predictions, set them to integer, update the `submission` dataframe, and save it as CSV... Oof!"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred =  pd.DataFrame(test_pred, columns=['preds'])\npred.plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This for loop its for round predictions\nsubmission['target'] = findClasses(test_pred)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}