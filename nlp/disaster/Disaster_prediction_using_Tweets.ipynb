{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Disaster prediction using Tweets",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Konerusudhir/kaggle_datasets/blob/master/nlp/disaster/Disaster_prediction_using_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYFxeSpCa-HU",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aCcBXGRpjm3",
        "colab_type": "code",
        "outputId": "82e8058f-b707-42b5-9094-b0654c2d1d11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# !pip uninstall -y tensorflow\n",
        "# !pip uninstall -y tf-nightly-gpu\n",
        "# !pip install tensorflow-gpu\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.preprocessing import sequence,text\n",
        "from tensorflow.python.keras import models, initializers, regularizers\n",
        "from tensorflow.python.keras.layers import  InputLayer, BatchNormalization,Dense, Dropout, Embedding, SeparableConv1D, MaxPooling1D, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import time\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import average_precision_score, auc, classification_report, confusion_matrix, roc_curve, precision_recall_curve, f1_score\n",
        "\n",
        "!pip install contractions\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.24)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OapZFeVRpzSp",
        "colab_type": "code",
        "outputId": "0643138b-29e2-4245-de5d-d0d9ece3f439",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# For finding Tensorflow version\n",
        "\n",
        "print(\"TF version: \", tf.__version__)\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"Is GPU available:     \" + str(tf.test.is_gpu_available()))\n",
        "print(\"GPU Name:  \" + tf.test.gpu_device_name())\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version:  2.0.0\n",
            "Hub version:  0.7.0\n",
            "Is GPU available:     True\n",
            "GPU Name:  /device:GPU:0\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 13782061350148272939\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 3030501206724426737\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 17748052595114363770\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15956161332\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 11931529481881581159\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tew8Bx0D9zKq",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4D4hZ1R8dBF",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW3aJMPRoqw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_training_and_validation_sets_with_shuffle(texts, labels, validation_split):\n",
        "    \"\"\"Splits the texts and labels into training and validation sets.\n",
        "    # Arguments\n",
        "        texts: list, text data.\n",
        "        labels: list, label data.\n",
        "        validation_split: float, percentage of data to use for validation.\n",
        "    # Returns\n",
        "        A tuple of training and validation data.\n",
        "    \"\"\"\n",
        "    num_training_samples = int((1 - validation_split) * len(texts))\n",
        "    X_train,X_test,y_train,y_test=train_test_split(\n",
        "        texts,\n",
        "        labels,\n",
        "        test_size=0.20, \n",
        "        random_state = 777, \n",
        "        shuffle = True\n",
        "    )\n",
        "    print('Shape of Train',X_train.shape)\n",
        "    print(\"Shape of Validation \",X_test.shape)\n",
        "\n",
        "    return ((X_train, y_train),\n",
        "            (X_test, y_test))\n",
        "\n",
        "def load_disaster_dataset(validation_split=0.2,\n",
        "                          seed=777):\n",
        "    \"\"\"Loads the tweet weather topic classification dataset.\n",
        "    # Arguments\n",
        "        data_path: string, path to the data directory.\n",
        "        validation_split: float, percentage of data to use for validation.\n",
        "        seed: int, seed for randomizer.\n",
        "    # Returns\n",
        "        A tuple of training and validation data.\n",
        "        Number of training samples: 62356\n",
        "        Number of test samples: 15590\n",
        "        Number of topics: 15\n",
        "    # References\n",
        "        https://www.kaggle.com/c/crowdflower-weather-twitter/data\n",
        "        Download from:\n",
        "        https://www.kaggle.com/c/3586/download/train.csv\n",
        "    \"\"\"\n",
        "\n",
        "    train = pd.read_csv(\"https://raw.githubusercontent.com/Konerusudhir/kaggle_datasets/master/nlp/disaster/train.csv\", error_bad_lines=False)\n",
        "    # test =  pd.read_csv(\"https://raw.githubusercontent.com/Konerusudhir/kaggle_datasets/master/nlp/disaster/test.csv\", error_bad_lines=False)\n",
        "    # submission = pd.read_csv(\"https://raw.githubusercontent.com/Konerusudhir/kaggle_datasets/master/nlp/disaster/sample_submission.csv\", error_bad_lines=False)\n",
        "\n",
        "\n",
        "    # columns = [1] + [i for i in range(13, 28)]  # 1 - text, 13-28 - topics.\n",
        "    # data = _load_and_shuffle_data(data_path, 'train.csv', columns, seed)\n",
        "\n",
        "    # # Get tweet text and the max confidence score for the weather types.\n",
        "    # texts = list(data['tweet'])\n",
        "    # weather_data = data.iloc[:, 1:]\n",
        "\n",
        "    # labels = []\n",
        "    # for i in range(len(texts)):\n",
        "    #     # Pick topic with the max confidence score.\n",
        "    #     labels.append(np.argmax(list(weather_data.iloc[i, :].values)))\n",
        "\n",
        "\n",
        "    text = train.text\n",
        "    labels = train.target\n",
        "    \n",
        "    return split_training_and_validation_sets_with_shuffle(\n",
        "        text, labels, validation_split)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKAHnWOH258n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKV4D9AF40XM",
        "colab_type": "text"
      },
      "source": [
        "## Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ5hwDyf410z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NGRAM_RANGE = (1, 2)\n",
        "\n",
        "# Limit on the number of features. We use the top 20K features.\n",
        "TOP_K = 20000\n",
        "\n",
        "# Whether text should be split into word or character n-grams.\n",
        "# One of 'word', 'char'.\n",
        "TOKEN_MODE = 'word'\n",
        "\n",
        "# Minimum document/corpus frequency below which a token will be discarded.\n",
        "MIN_DOCUMENT_FREQUENCY = 2\n",
        "\n",
        "# Limit on the length of text sequences. Sequences longer than this\n",
        "# will be truncated.\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "\n",
        "\n",
        "def get_num_classes(labels):\n",
        "    \"\"\"Gets the total number of classes.\n",
        "    # Arguments\n",
        "        labels: list, label values.\n",
        "            There should be at lease one sample for values in the\n",
        "            range (0, num_classes -1)\n",
        "    # Returns\n",
        "        int, total number of classes.\n",
        "    # Raises\n",
        "        ValueError: if any label value in the range(0, num_classes - 1)\n",
        "            is missing or if number of classes is <= 1.\n",
        "    \"\"\"\n",
        "    num_classes = max(labels) + 1\n",
        "    missing_classes = [i for i in range(num_classes) if i not in labels]\n",
        "    if len(missing_classes):\n",
        "        raise ValueError('Missing samples with label value(s) '\n",
        "                         '{missing_classes}. Please make sure you have '\n",
        "                         'at least one sample for every label value '\n",
        "                         'in the range(0, {max_class})'.format(\n",
        "                            missing_classes=missing_classes,\n",
        "                            max_class=num_classes - 1))\n",
        "\n",
        "    if num_classes <= 1:\n",
        "        raise ValueError('Invalid number of labels: {num_classes}.'\n",
        "                         'Please make sure there are at least two classes '\n",
        "                         'of samples'.format(num_classes=num_classes))\n",
        "    return num_classes\n",
        "\n",
        "\n",
        "def get_num_words_per_sample(sample_texts):\n",
        "    \"\"\"Gets the median number of words per sample given corpus.\n",
        "    # Arguments\n",
        "        sample_texts: list, sample texts.\n",
        "    # Returns\n",
        "        int, median number of words per sample.\n",
        "    \"\"\"\n",
        "    num_words = [len(s.split()) for s in sample_texts]\n",
        "    return np.median(num_words)\n",
        "\n",
        "\n",
        "def plot_frequency_distribution_of_ngrams(sample_texts,\n",
        "                                          ngram_range=(1, 2),\n",
        "                                          num_ngrams=50):\n",
        "    \"\"\"Plots the frequency distribution of n-grams.\n",
        "    # Arguments\n",
        "        samples_texts: list, sample texts.\n",
        "        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n",
        "            Min and mplt are the lower and upper bound values for the range.\n",
        "        num_ngrams: int, number of n-grams to plot.\n",
        "            Top `num_ngrams` frequent n-grams will be plotted.\n",
        "    \"\"\"\n",
        "    # Create args required for vectorizing.\n",
        "    kwargs = {\n",
        "            'ngram_range': (1, 1),\n",
        "            'dtype': 'int32',\n",
        "            'strip_accents': 'unicode',\n",
        "            'decode_error': 'replace',\n",
        "            'analyzer': 'word',  # Split text into word tokens.\n",
        "    }\n",
        "    vectorizer = CountVectorizer(**kwargs)\n",
        "\n",
        "    # This creates a vocabulary (dict, where keys are n-grams and values are\n",
        "    # idxices). This also converts every text to an array the length of\n",
        "    # vocabulary, where every element idxicates the count of the n-gram\n",
        "    # corresponding at that idxex in vocabulary.\n",
        "    vectorized_texts = vectorizer.fit_transform(sample_texts)\n",
        "\n",
        "    # This is the list of all n-grams in the index order from the vocabulary.\n",
        "    all_ngrams = list(vectorizer.get_feature_names())\n",
        "    num_ngrams = min(num_ngrams, len(all_ngrams))\n",
        "    # ngrams = all_ngrams[:num_ngrams]\n",
        "\n",
        "    # Add up the counts per n-gram ie. column-wise\n",
        "    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n",
        "\n",
        "    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n",
        "    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n",
        "        zip(all_counts, all_ngrams), reverse=True)])\n",
        "    ngrams = list(all_ngrams)[:num_ngrams]\n",
        "    counts = list(all_counts)[:num_ngrams]\n",
        "\n",
        "    idx = np.arange(num_ngrams)\n",
        "    plt.bar(idx, counts, width=0.8, color='b')\n",
        "    plt.xlabel('N-grams')\n",
        "    plt.ylabel('Frequencies')\n",
        "    plt.title('Frequency distribution of n-grams')\n",
        "    plt.xticks(idx, ngrams, rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_sample_length_distribution(sample_texts):\n",
        "    \"\"\"Plots the sample length distribution.\n",
        "    # Arguments\n",
        "        samples_texts: list, sample texts.\n",
        "    \"\"\"\n",
        "    plt.hist([len(s) for s in sample_texts], 50)\n",
        "    plt.xlabel('Length of a sample')\n",
        "    plt.ylabel('Number of samples')\n",
        "    plt.title('Sample length distribution')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_class_distribution(labels):\n",
        "    \"\"\"Plots the class distribution.\n",
        "    # Arguments\n",
        "        labels: list, label values.\n",
        "            There should be at lease one sample for values in the\n",
        "            range (0, num_classes -1)\n",
        "    \"\"\"\n",
        "    num_classes = get_num_classes(labels)\n",
        "    count_map = Counter(labels)\n",
        "    counts = [count_map[i] for i in range(num_classes)]\n",
        "    idx = np.arange(num_classes)\n",
        "    plt.bar(idx, counts, width=0.8, color='b')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Number of samples')\n",
        "    plt.title('Class distribution')\n",
        "    plt.xticks(idx, idx)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rWkIskWLHs5",
        "colab_type": "text"
      },
      "source": [
        "## Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBpBAg4RH5nn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def pre_process_document(document):\n",
        "    # strip HTML\n",
        "    document = strip_html_tags(document)\n",
        "    # lower case\n",
        "    document = document.lower()\n",
        "    # remove extra newlines (often might be present in really noisy text)\n",
        "    document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    # remove accented characters\n",
        "    document = remove_accented_chars(document)\n",
        "    # expand contractions    \n",
        "    document = expand_contractions(document)  \n",
        "    # remove special characters and\\or digits    \n",
        "    # insert spaces between special characters to isolate them    \n",
        "    special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "    document = special_char_pattern.sub(\" \\\\1 \", document)\n",
        "    document = remove_special_characters(document, remove_digits=True)  \n",
        "    # remove extra whitespace\n",
        "    document = re.sub(' +', ' ', document)\n",
        "    document = document.strip()\n",
        "    \n",
        "    return document\n",
        "\n",
        "def clean_data(data):\n",
        "    ((X_train, y_train), (X_test, y_test)) = data \n",
        "    pre_process_corpus = np.vectorize(pre_process_document)\n",
        "    cleaned_train_text = pre_process_corpus(X_train)\n",
        "    cleaned_test_text = pre_process_corpus(X_test)\n",
        "    return ((cleaned_train_text, y_train), (cleaned_test_text, y_test))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS69j3xH61uP",
        "colab_type": "text"
      },
      "source": [
        "# Build All Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMWisB0x43Q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
        "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
        "    # Arguments\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of the layers.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "    # Returns\n",
        "        An MLP model instance.\n",
        "    \"\"\"\n",
        "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
        "    model = models.Sequential()\n",
        "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
        "\n",
        "    for _ in range(layers-1):\n",
        "        model.add(Dense(units=units, activation='relu'))\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    model.add(Dense(units=op_units, activation=op_activation))\n",
        "    return model\n",
        "\n",
        "\n",
        "def sepcnn_model(blocks,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 embedding_dim,\n",
        "                 dropout_rate,\n",
        "                 pool_size,\n",
        "                 input_shape,\n",
        "                 num_classes,\n",
        "                 num_features,\n",
        "                 use_pretrained_embedding=False,\n",
        "                 is_embedding_trainable=False,\n",
        "                 embedding_matrix=None):\n",
        "    \"\"\"Creates an instance of a separable CNN model.\n",
        "    # Arguments\n",
        "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
        "        filters: int, output dimension of the layers.\n",
        "        kernel_size: int, length of the convolution window.\n",
        "        embedding_dim: int, dimension of the embedding vectors.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "        num_features: int, number of words (embedding input dimension).\n",
        "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
        "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
        "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
        "    # Returns\n",
        "        A sepCNN model instance.\n",
        "    \"\"\"\n",
        "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
        "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
        "    if use_pretrained_embedding:\n",
        "        model.add(Embedding(input_dim=num_features,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=input_shape[0],\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=is_embedding_trainable))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim=num_features,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=input_shape[0]))\n",
        "\n",
        "    for _ in range(blocks-1):\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "        model.add(SeparableConv1D(filters=filters,\n",
        "                                  kernel_size=kernel_size,\n",
        "                                  activation='relu',\n",
        "                                  bias_initializer='random_uniform',\n",
        "                                  depthwise_initializer='random_uniform',\n",
        "                                  padding='same'))\n",
        "        model.add(SeparableConv1D(filters=filters,\n",
        "                                  kernel_size=kernel_size,\n",
        "                                  activation='relu',\n",
        "                                  bias_initializer='random_uniform',\n",
        "                                  depthwise_initializer='random_uniform',\n",
        "                                  padding='same'))\n",
        "        model.add(MaxPooling1D(pool_size=pool_size))\n",
        "\n",
        "    model.add(SeparableConv1D(filters=filters * 2,\n",
        "                              kernel_size=kernel_size,\n",
        "                              activation='relu',\n",
        "                              bias_initializer='random_uniform',\n",
        "                              depthwise_initializer='random_uniform',\n",
        "                              padding='same'))\n",
        "    model.add(SeparableConv1D(filters=filters * 2,\n",
        "                              kernel_size=kernel_size,\n",
        "                              activation='relu',\n",
        "                              bias_initializer='random_uniform',\n",
        "                              depthwise_initializer='random_uniform',\n",
        "                              padding='same'))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "    model.add(Dense(op_units, activation=op_activation))\n",
        "    return model\n",
        "\n",
        "\n",
        "def _get_last_layer_units_and_activation(num_classes):\n",
        "    \"\"\"Gets the # units and activation function for the last network layer.\n",
        "    # Arguments\n",
        "        num_classes: int, number of classes.\n",
        "    # Returns\n",
        "        units, activation values.\n",
        "    \"\"\"\n",
        "    if num_classes == 2:\n",
        "        activation = 'sigmoid'\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = 'softmax'\n",
        "        units = num_classes\n",
        "    return units, activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzAlLwElQwnI",
        "colab_type": "text"
      },
      "source": [
        "## Smaller SepCNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOHdVUcHQ08n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sepcnn_model_smaller(blocks,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 embedding_dim,\n",
        "                 dropout_rate,\n",
        "                 pool_size,\n",
        "                 input_shape,\n",
        "                 num_classes,\n",
        "                 num_features,\n",
        "                 use_pretrained_embedding=False,\n",
        "                 is_embedding_trainable=False,\n",
        "                 embedding_matrix=None):\n",
        "    \"\"\"Creates an instance of a separable CNN model.\n",
        "    # Arguments\n",
        "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
        "        filters: int, output dimension of the layers.\n",
        "        kernel_size: int, length of the convolution window.\n",
        "        embedding_dim: int, dimension of the embedding vectors.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "        num_features: int, number of words (embedding input dimension).\n",
        "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
        "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
        "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
        "    # Returns\n",
        "        A sepCNN model instance.\n",
        "    \"\"\"\n",
        "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
        "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
        "    if use_pretrained_embedding:\n",
        "        model.add(Embedding(input_dim=num_features,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=input_shape[0],\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=is_embedding_trainable))\n",
        "    else:\n",
        "        model.add(Embedding(input_dim=num_features,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=input_shape[0]))\n",
        "\n",
        "    for _ in range(blocks-1):\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "        model.add(SeparableConv1D(filters=filters,\n",
        "                                  kernel_size=kernel_size,\n",
        "                                  activation='relu',\n",
        "                                  bias_initializer='random_uniform',\n",
        "                                  depthwise_initializer='random_uniform',\n",
        "                                  padding='same'))\n",
        "        model.add(MaxPooling1D(pool_size=pool_size))\n",
        "\n",
        "    model.add(SeparableConv1D(filters=filters,\n",
        "                              kernel_size=kernel_size,\n",
        "                              activation='relu',\n",
        "                              bias_initializer='random_uniform',\n",
        "                              depthwise_initializer='random_uniform',\n",
        "                              padding='same'))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "    model.add(Dense(op_units, activation=op_activation))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCUp4d0wWzA2",
        "colab_type": "text"
      },
      "source": [
        "## USE Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeNbS7JuW1u8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-large/4'\n",
        "use_embedding_layer = hub.KerasLayer(module_url, trainable=False, name='USE_embedding')\n",
        "with tf.device('/gpu:0'):\n",
        "    def build_use_model(train_use_layer=False):\n",
        "        model = Sequential()\n",
        "        model.add(InputLayer(input_shape=[], dtype=tf.string))     \n",
        "        use_embedding_layer.trainable = train_use_layer \n",
        "        model.add(use_embedding_layer)\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.001)))\n",
        "        model.summary()        \n",
        "\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQvQl2Eu9ZSk",
        "colab_type": "text"
      },
      "source": [
        "# Vectorize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSFvfhUw662y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorization parameters\n",
        "\n",
        "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
        "NGRAM_RANGE = (1, 2)\n",
        "\n",
        "# Limit on the number of features. We use the top 20K features.\n",
        "TOP_K = 20000\n",
        "\n",
        "# Whether text should be split into word or character n-grams.\n",
        "# One of 'word', 'char'.\n",
        "TOKEN_MODE = 'word'\n",
        "\n",
        "# Minimum document/corpus frequency below which a token will be discarded.\n",
        "MIN_DOCUMENT_FREQUENCY = 2\n",
        "\n",
        "# Limit on the length of text sequences. Sequences longer than this\n",
        "# will be truncated.\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "\n",
        "\n",
        "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
        "    \"\"\"Vectorizes texts as ngram vectors.\n",
        "    1 text = 1 tf-idf vector the length of vocabulary of uni-grams + bi-grams.\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        train_labels: np.ndarray, training labels.\n",
        "        val_texts: list, validation text strings.\n",
        "    # Returns\n",
        "        x_train, x_val: vectorized training and validation texts\n",
        "    \"\"\"\n",
        "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
        "    kwargs = {\n",
        "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
        "            'dtype': 'int32',\n",
        "            'strip_accents': 'unicode',\n",
        "            'decode_error': 'replace',\n",
        "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
        "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
        "    }\n",
        "    vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "    # Learn vocabulary from training texts and vectorize training texts.\n",
        "    x_train = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "    # Vectorize validation texts.\n",
        "    x_val = vectorizer.transform(val_texts)\n",
        "\n",
        "    # Select top 'k' of the vectorized features.\n",
        "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
        "    selector.fit(x_train, train_labels)\n",
        "    x_train = selector.transform(x_train)\n",
        "    x_val = selector.transform(x_val)\n",
        "\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_val = x_val.astype('float32')\n",
        "    return x_train, x_val\n",
        "\n",
        "\n",
        "def sequence_vectorize(train_texts, val_texts):\n",
        "    \"\"\"Vectorizes texts as sequence vectors.\n",
        "    1 text = 1 sequence vector with fixed length.\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        val_texts: list, validation text strings.\n",
        "    # Returns\n",
        "        x_train, x_val, word_index: vectorized training and validation\n",
        "            texts and word index dictionary.\n",
        "    \"\"\"\n",
        "    # Create vocabulary with training texts.\n",
        "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
        "    tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "    # Vectorize training and validation texts.\n",
        "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
        "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
        "\n",
        "    # Get max sequence length.\n",
        "    max_length = len(max(x_train, key=len))\n",
        "    if max_length > MAX_SEQUENCE_LENGTH:\n",
        "        max_length = MAX_SEQUENCE_LENGTH\n",
        "\n",
        "    # Fix sequence length to max value. Sequences shorter than the length are\n",
        "    # padded in the beginning and sequences longer are truncated\n",
        "    # at the beginning.\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
        "    return x_train, x_val, tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qAIk4-Rz5tw",
        "colab_type": "text"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBNbjwT6z9JI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomMetrics(tf.keras.callbacks.Callback):\n",
        "\n",
        "    # parameterized constructor \n",
        "    def __init__(self,  val_texts, val_labels, model_name): \n",
        "        self.val_texts = tf.convert_to_tensor(val_texts, dtype=tf.string)\n",
        "        self.val_labels = tf.convert_to_tensor(val_labels, dtype=tf.bool)\n",
        "        self.model_name = model_name\n",
        "        self.max_f1_score = 0.0\n",
        "\n",
        "    def on_train_begin(self, logs={}):       \n",
        "        self.f1s = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):       \n",
        "        if(epoch%5 == 0):\n",
        "            score = np.asarray(self.model.predict(self.val_texts))\n",
        "            predict = np.round(score)\n",
        "\n",
        "            # self.accuracy.append(sklm.accuracy_score(y_test, predict))\n",
        "            # self.precision.append(sklm.precision_score(y_test, predict))\n",
        "            # self.recall.append(sklm.recall_score(y_test, predict))\n",
        "            current_fl_score = f1_score(self.val_labels, predict)\n",
        "            if(current_fl_score - self.max_f1_score > 0.0001): \n",
        "                # self.f1s.append(fl_score)\n",
        "                logs[\"val_fl_score\"] = current_fl_score\n",
        "                print(\" - val_f1_score: \", str(current_fl_score))\n",
        "                self.model.save_weights(self.model_name)\n",
        "                self.max_f1_score = current_fl_score\n",
        "\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_Ih4zfr_Muo",
        "colab_type": "text"
      },
      "source": [
        "# Train Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9d3ZWRL7oZj",
        "colab_type": "text"
      },
      "source": [
        "## Train N Gram Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJjdhzKg8A9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_ngram_model(data,\n",
        "                      learning_rate=1e-3,\n",
        "                      epochs=1000,\n",
        "                      batch_size=128,\n",
        "                      layers=2,\n",
        "                      units=64,\n",
        "                      dropout_rate=0.2):\n",
        "    \"\"\"Trains n-gram model on the given dataset.\n",
        "    # Arguments\n",
        "        data: tuples of training and test texts and labels.\n",
        "        learning_rate: float, learning rate for training model.\n",
        "        epochs: int, number of epochs.\n",
        "        batch_size: int, number of samples per batch.\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of Dense layers in the model.\n",
        "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "    # Raises\n",
        "        ValueError: If validation data has label values which were not seen\n",
        "            in the training data.\n",
        "    \"\"\"\n",
        "    # Get the data.\n",
        "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
        "\n",
        "    # Verify that validation labels are in the same range as training labels.\n",
        "    num_classes = get_num_classes(train_labels)\n",
        "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
        "    if len(unexpected_labels):\n",
        "        raise ValueError('Unexpected label values found in the validation set:'\n",
        "                         ' {unexpected_labels}. Please make sure that the '\n",
        "                         'labels in the validation set are in the same range '\n",
        "                         'as training labels.'.format(\n",
        "                             unexpected_labels=unexpected_labels))\n",
        "\n",
        "    # Vectorize texts.\n",
        "    x_train, x_val = ngram_vectorize(\n",
        "        train_texts, train_labels, val_texts)\n",
        "\n",
        "    # Create model instance.\n",
        "    model = mlp_model(layers=layers,\n",
        "                                  units=units,\n",
        "                                  dropout_rate=dropout_rate,\n",
        "                                  input_shape=x_train.shape[1:],\n",
        "                                  num_classes=num_classes)\n",
        "\n",
        "    # Compile model with learning parameters.\n",
        "    if num_classes == 2:\n",
        "        loss = 'binary_crossentropy'\n",
        "    else:\n",
        "        loss = 'sparse_categorical_crossentropy'\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "    # Create callback for early stopping on validation loss. If the loss does\n",
        "    # not decrease in two consecutive tries, stop training.\n",
        "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=2)]\n",
        "\n",
        "    # Train and validate model.\n",
        "    history = model.fit(\n",
        "            x_train,\n",
        "            train_labels,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            validation_data=(x_val, val_labels),\n",
        "            verbose=2,  # Logs once per epoch.\n",
        "            batch_size=batch_size)\n",
        "\n",
        "    # Print results.\n",
        "    history = history.history\n",
        "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "    # Save model.\n",
        "    model.save('imdb_mlp_model.h5')\n",
        "    return history['val_acc'][-1], history['val_loss'][-1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyuCd5SX_HkQ",
        "colab_type": "text"
      },
      "source": [
        "## Train Sequence Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVRjrpHk8V7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOP_K = 20000\n",
        "\n",
        "def train_sequence_model(data,\n",
        "                         learning_rate=1e-3,\n",
        "                         epochs=1000,\n",
        "                         batch_size=128,\n",
        "                         blocks=2,\n",
        "                         filters=64,\n",
        "                         dropout_rate=0.2,\n",
        "                         embedding_dim=200,\n",
        "                         kernel_size=3,\n",
        "                         pool_size=3):\n",
        "    \"\"\"Trains sequence model on the given dataset.\n",
        "    # Arguments\n",
        "        data: tuples of training and test texts and labels.\n",
        "        learning_rate: float, learning rate for training model.\n",
        "        epochs: int, number of epochs.\n",
        "        batch_size: int, number of samples per batch.\n",
        "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
        "        filters: int, output dimension of sepCNN layers in the model.\n",
        "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "        embedding_dim: int, dimension of the embedding vectors.\n",
        "        kernel_size: int, length of the convolution window.\n",
        "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
        "    # Raises\n",
        "        ValueError: If validation data has label values which were not seen\n",
        "            in the training data.\n",
        "    \"\"\"\n",
        "    # Get the data.\n",
        "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
        "\n",
        "    # Verify that validation labels are in the same range as training labels.\n",
        "    num_classes = get_num_classes(train_labels)\n",
        "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
        "    if len(unexpected_labels):\n",
        "        raise ValueError('Unexpected label values found in the validation set:'\n",
        "                         ' {unexpected_labels}. Please make sure that the '\n",
        "                         'labels in the validation set are in the same range '\n",
        "                         'as training labels.'.format(\n",
        "                             unexpected_labels=unexpected_labels))\n",
        "\n",
        "    # Vectorize texts.\n",
        "    x_train, x_val, word_index = sequence_vectorize(\n",
        "            train_texts, val_texts)\n",
        "\n",
        "    # Number of features will be the embedding input dimension. Add 1 for the\n",
        "    # reserved index 0.\n",
        "    num_features = min(len(word_index) + 1, TOP_K)\n",
        "\n",
        "    # Create model instance.\n",
        "    model = sepcnn_model(blocks=blocks,\n",
        "                                     filters=filters,\n",
        "                                     kernel_size=kernel_size,\n",
        "                                     embedding_dim=embedding_dim,\n",
        "                                     dropout_rate=dropout_rate,\n",
        "                                     pool_size=pool_size,\n",
        "                                     input_shape=x_train.shape[1:],\n",
        "                                     num_classes=num_classes,\n",
        "                                     num_features=num_features)\n",
        "\n",
        "    # Compile model with learning parameters.\n",
        "    if num_classes == 2:\n",
        "        loss = 'binary_crossentropy'\n",
        "    else:\n",
        "        loss = 'sparse_categorical_crossentropy'\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "    # Model Summary\n",
        "    model.summary()\n",
        "    \n",
        "    # Create callback for early stopping on validation loss. If the loss does\n",
        "    # not decrease in two consecutive tries, stop training.\n",
        "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=2)]\n",
        "\n",
        "    # Train and validate model.\n",
        "    history = model.fit(\n",
        "            x_train,\n",
        "            train_labels,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            validation_data=(x_val, val_labels),\n",
        "            verbose=2,  # Logs once per epoch.\n",
        "            batch_size=batch_size)\n",
        "\n",
        "    # Print results.\n",
        "    history = history.history\n",
        "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "    # Save model.\n",
        "    model.save('rotten_tomatoes_sepcnn_model.h5')\n",
        "    return history['val_acc'][-1], history['val_loss'][-1]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fs3KOzcRVms",
        "colab_type": "text"
      },
      "source": [
        "## Load Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PBH5u7FRZIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding_matrix(word_index, embedding_data_dir, embedding_dim):\n",
        "    \"\"\"Gets embedding matrix from the embedding index data.\n",
        "    # Arguments\n",
        "        word_index: dict, word to index map that was generated from the data.\n",
        "        embedding_data_dir: string, path to the pre-training embeddings.\n",
        "        embedding_dim: int, dimension of the embedding vectors.\n",
        "    # Returns\n",
        "        dict, word vectors for words in word_index from pre-trained embedding.\n",
        "    # References:\n",
        "        https://nlp.stanford.edu/projects/glove/\n",
        "        Download and uncompress archive from:\n",
        "        http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    \"\"\"\n",
        "\n",
        "    # Read the pre-trained embedding file and get word to word vector mappings.\n",
        "    embedding_matrix_all = {}\n",
        "\n",
        "    # We are using 200d GloVe embeddings.\n",
        "    fname = os.path.join(embedding_data_dir, 'glove.twitter.27B.25d.txt')\n",
        "    with open(fname) as f:\n",
        "        for line in f:  # Every line contains word followed by the vector value\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embedding_matrix_all[word] = coefs\n",
        "\n",
        "    # Prepare embedding matrix with just the words in our word_index dictionary\n",
        "    num_words = min(len(word_index) + 1, TOP_K)\n",
        "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if i >= TOP_K:\n",
        "            continue\n",
        "        embedding_vector = embedding_matrix_all.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOHyR85t_5kc",
        "colab_type": "text"
      },
      "source": [
        "## Train Fine Tuned Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59Xxc2lT__Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOP_K = 20000\n",
        "\n",
        "def train_fine_tuned_sequence_model(data,\n",
        "                                    embedding_data_dir,\n",
        "                                    learning_rate=1e-3,\n",
        "                                    epochs=1000,\n",
        "                                    batch_size=128,\n",
        "                                    blocks=2,\n",
        "                                    filters=64,\n",
        "                                    dropout_rate=0.2,\n",
        "                                    embedding_dim=25,\n",
        "                                    kernel_size=3,\n",
        "                                    pool_size=3):\n",
        "    \"\"\"Trains sequence model on the given dataset.\n",
        "    # Arguments\n",
        "        data: tuples of training and test texts and labels.\n",
        "        embedding_data_dir: string, path to the pre-training embeddings.\n",
        "        learning_rate: float, learning rate for training model.\n",
        "        epochs: int, number of epochs.\n",
        "        batch_size: int, number of samples per batch.\n",
        "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
        "        filters: int, output dimension of sepCNN layers in the model.\n",
        "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "        embedding_dim: int, dimension of the embedding vectors.\n",
        "        kernel_size: int, length of the convolution window.\n",
        "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
        "    # Raises\n",
        "        ValueError: If validation data has label values which were not seen\n",
        "            in the training data.\n",
        "    \"\"\"\n",
        "    # Get the data.\n",
        "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
        "\n",
        "    # Verify that validation labels are in the same range as training labels.\n",
        "    num_classes = get_num_classes(train_labels)\n",
        "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
        "    if len(unexpected_labels):\n",
        "        raise ValueError('Unexpected label values found in the validation set:'\n",
        "                         ' {unexpected_labels}. Please make sure that the '\n",
        "                         'labels in the validation set are in the same range '\n",
        "                         'as training labels.'.format(\n",
        "                             unexpected_labels=unexpected_labels))\n",
        "\n",
        "    # Vectorize texts.\n",
        "    x_train, x_val, word_index = sequence_vectorize(\n",
        "            train_texts, val_texts)\n",
        "\n",
        "    # Number of features will be the embedding input dimension. Add 1 for the\n",
        "    # reserved index 0.\n",
        "    num_features = min(len(word_index) + 1, TOP_K)\n",
        "\n",
        "    embedding_matrix = get_embedding_matrix(\n",
        "        word_index, embedding_data_dir, embedding_dim)\n",
        "\n",
        "    # Create model instance. First time we will train rest of network while\n",
        "    # keeping embedding layer weights frozen. So, we set\n",
        "    # is_embedding_trainable as False.\n",
        "    model = sepcnn_model(blocks=blocks,\n",
        "                                     filters=filters,\n",
        "                                     kernel_size=kernel_size,\n",
        "                                     embedding_dim=embedding_dim,\n",
        "                                     dropout_rate=dropout_rate,\n",
        "                                     pool_size=pool_size,\n",
        "                                     input_shape=x_train.shape[1:],\n",
        "                                     num_classes=num_classes,\n",
        "                                     num_features=num_features,\n",
        "                                     use_pretrained_embedding=True,\n",
        "                                     is_embedding_trainable=False,\n",
        "                                     embedding_matrix=embedding_matrix)\n",
        "\n",
        "    # Model Summary\n",
        "    model.summary()\n",
        "\n",
        "    # Compile model with learning parameters.\n",
        "    if num_classes == 2:\n",
        "        loss = 'binary_crossentropy'\n",
        "    else:\n",
        "        loss = 'sparse_categorical_crossentropy'\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "    # Create callback for early stopping on validation loss. If the loss does\n",
        "    # not decrease in two consecutive tries, stop training.\n",
        "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=2)]\n",
        "\n",
        "    # Train and validate model.\n",
        "    model.fit(x_train,\n",
        "              train_labels,\n",
        "              epochs=epochs,\n",
        "              callbacks=callbacks,\n",
        "              validation_data=(x_val, val_labels),\n",
        "              verbose=2,  # Logs once per epoch.\n",
        "              batch_size=batch_size)\n",
        "\n",
        "    # Save the model.\n",
        "    model.save_weights('sequence_model_with_pre_trained_embedding.h5')\n",
        "\n",
        "    # Create another model instance. This time we will unfreeze the embedding\n",
        "    # layer and let it fine-tune to the given dataset.\n",
        "    model = sepcnn_model(blocks=blocks,\n",
        "                                     filters=filters,\n",
        "                                     kernel_size=kernel_size,\n",
        "                                     embedding_dim=embedding_dim,\n",
        "                                     dropout_rate=dropout_rate,\n",
        "                                     pool_size=pool_size,\n",
        "                                     input_shape=x_train.shape[1:],\n",
        "                                     num_classes=num_classes,\n",
        "                                     num_features=num_features,\n",
        "                                     use_pretrained_embedding=True,\n",
        "                                     is_embedding_trainable=True,\n",
        "                                     embedding_matrix=embedding_matrix)\n",
        "\n",
        "    # Compile model with learning parameters.\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "    # Load the weights that we had saved into this new model.\n",
        "    model.load_weights('sequence_model_with_pre_trained_embedding.h5')\n",
        "\n",
        "    # Train and validate model.\n",
        "    history = model.fit(x_train,\n",
        "                        train_labels,\n",
        "                        epochs=epochs,\n",
        "                        callbacks=callbacks,\n",
        "                        validation_data=(x_val, val_labels),\n",
        "                        verbose=2,  # Logs once per epoch.\n",
        "                        batch_size=batch_size)\n",
        "\n",
        "    # Print results.\n",
        "    history = history.history\n",
        "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "    # Save model.\n",
        "    model.save('tweet_weather_sepcnn_fine_tuned_model.h5')\n",
        "    return history['val_acc'][-1], history['val_loss'][-1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYXm7D3XSMQk",
        "colab_type": "text"
      },
      "source": [
        "## Train Smaller Fine Tune Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu8WD-kBSSpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOP_K = 20000\n",
        "\n",
        "def train_smaller_fine_tuned_sequence_model(data,\n",
        "                                    embedding_data_dir,\n",
        "                                    learning_rate=1e-3,\n",
        "                                    epochs=1000,\n",
        "                                    batch_size=128,\n",
        "                                    blocks=2,\n",
        "                                    filters=64,\n",
        "                                    dropout_rate=0.2,\n",
        "                                    embedding_dim=25,\n",
        "                                    kernel_size=3,\n",
        "                                    pool_size=3):\n",
        "    \"\"\"Trains sequence model on the given dataset.\n",
        "    # Arguments\n",
        "        data: tuples of training and test texts and labels.\n",
        "        embedding_data_dir: string, path to the pre-training embeddings.\n",
        "        learning_rate: float, learning rate for training model.\n",
        "        epochs: int, number of epochs.\n",
        "        batch_size: int, number of samples per batch.\n",
        "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
        "        filters: int, output dimension of sepCNN layers in the model.\n",
        "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "        embedding_dim: int, dimension of the embedding vectors.\n",
        "        kernel_size: int, length of the convolution window.\n",
        "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
        "    # Raises\n",
        "        ValueError: If validation data has label values which were not seen\n",
        "            in the training data.\n",
        "    \"\"\"\n",
        "    # Get the data.\n",
        "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
        "\n",
        "    # Verify that validation labels are in the same range as training labels.\n",
        "    num_classes = get_num_classes(train_labels)\n",
        "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
        "    if len(unexpected_labels):\n",
        "        raise ValueError('Unexpected label values found in the validation set:'\n",
        "                         ' {unexpected_labels}. Please make sure that the '\n",
        "                         'labels in the validation set are in the same range '\n",
        "                         'as training labels.'.format(\n",
        "                             unexpected_labels=unexpected_labels))\n",
        "\n",
        "    # Vectorize texts.\n",
        "    x_train, x_val, word_index = sequence_vectorize(\n",
        "            train_texts, val_texts)\n",
        "\n",
        "    # Number of features will be the embedding input dimension. Add 1 for the\n",
        "    # reserved index 0.\n",
        "    num_features = min(len(word_index) + 1, TOP_K)\n",
        "\n",
        "    embedding_matrix = get_embedding_matrix(\n",
        "        word_index, embedding_data_dir, embedding_dim)\n",
        "\n",
        "    # Create model instance. First time we will train rest of network while\n",
        "    # keeping embedding layer weights frozen. So, we set\n",
        "    # is_embedding_trainable as False.\n",
        "    model = sepcnn_model_smaller(blocks=blocks,\n",
        "                                     filters=filters,\n",
        "                                     kernel_size=kernel_size,\n",
        "                                     embedding_dim=embedding_dim,\n",
        "                                     dropout_rate=dropout_rate,\n",
        "                                     pool_size=pool_size,\n",
        "                                     input_shape=x_train.shape[1:],\n",
        "                                     num_classes=num_classes,\n",
        "                                     num_features=num_features,\n",
        "                                     use_pretrained_embedding=True,\n",
        "                                     is_embedding_trainable=False,\n",
        "                                     embedding_matrix=embedding_matrix)\n",
        "\n",
        "    # Model Summary\n",
        "    model.summary()\n",
        "\n",
        "    # Compile model with learning parameters.\n",
        "    if num_classes == 2:\n",
        "        loss = 'binary_crossentropy'\n",
        "    else:\n",
        "        loss = 'sparse_categorical_crossentropy'\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "    metrics = [\n",
        "        'accuracy', \n",
        "        tf.keras.metrics.Recall(),\n",
        "        tf.keras.metrics.Precision()\n",
        "    ]\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    # Create callback for early stopping on validation loss. If the loss does\n",
        "    # not decrease in two consecutive tries, stop training.\n",
        "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=2)]\n",
        "\n",
        "    # Train and validate model.\n",
        "    model.fit(x_train,\n",
        "              train_labels,\n",
        "              epochs=epochs,\n",
        "              callbacks=callbacks,\n",
        "              validation_data=(x_val, val_labels),\n",
        "              verbose=2,  # Logs once per epoch.\n",
        "              batch_size=batch_size)\n",
        "\n",
        "    # Save the model.\n",
        "    model.save_weights('smaller_sequence_model_with_pre_trained_embedding.h5')\n",
        "\n",
        "    # Create another model instance. This time we will unfreeze the embedding\n",
        "    # layer and let it fine-tune to the given dataset.\n",
        "    model = sepcnn_model_smaller(blocks=blocks,\n",
        "                                     filters=filters,\n",
        "                                     kernel_size=kernel_size,\n",
        "                                     embedding_dim=embedding_dim,\n",
        "                                     dropout_rate=dropout_rate,\n",
        "                                     pool_size=pool_size,\n",
        "                                     input_shape=x_train.shape[1:],\n",
        "                                     num_classes=num_classes,\n",
        "                                     num_features=num_features,\n",
        "                                     use_pretrained_embedding=True,\n",
        "                                     is_embedding_trainable=True,\n",
        "                                     embedding_matrix=embedding_matrix)\n",
        "\n",
        "    # Compile model with learning parameters.\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "    # Load the weights that we had saved into this new model.\n",
        "    model.load_weights('smaller_sequence_model_with_pre_trained_embedding.h5')\n",
        "\n",
        "    # Train and validate model.\n",
        "    history = model.fit(x_train,\n",
        "                        train_labels,\n",
        "                        epochs=epochs,\n",
        "                        callbacks=callbacks,\n",
        "                        validation_data=(x_val, val_labels),\n",
        "                        verbose=2,  # Logs once per epoch.\n",
        "                        batch_size=batch_size)\n",
        "\n",
        "    # Print results.\n",
        "    history = history.history\n",
        "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "    # Save model.\n",
        "    model.save('smaller_sequence_model_with_pre_trained_embedding.h5')\n",
        "    return history['val_acc'][-1], history['val_loss'][-1]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtBFhAVZX-5u",
        "colab_type": "text"
      },
      "source": [
        "## Train USE Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-5YaGmlYCjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_use_model(data,\n",
        "                    batch_size=64,\n",
        "                    epochs=20):\n",
        "\n",
        "    # Get the data.\n",
        "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
        "\n",
        "    train_texts = tf.convert_to_tensor(train_texts.values, dtype=tf.string)\n",
        "    train_labels = tf.convert_to_tensor(train_labels.values, dtype=tf.bool)\n",
        "    val_texts = tf.convert_to_tensor(val_texts.values, dtype=tf.string)\n",
        "    val_labels = tf.convert_to_tensor(val_labels.values, dtype=tf.bool)\n",
        "\n",
        "    model = build_use_model(train_use_layer = False)\n",
        "\n",
        "    optimizer = Adam(lr = 0.0001)\n",
        "    metrics = [\n",
        "        'accuracy', \n",
        "        tf.keras.metrics.Recall(),\n",
        "        tf.keras.metrics.Precision()\n",
        "    ]\n",
        "    \n",
        "    model.compile(optimizer, loss='binary_crossentropy', metrics=metrics)  \n",
        "\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        'model_use_low_val_loss.h5', \n",
        "        monitor='val_loss', \n",
        "        mode='min', \n",
        "        modelsave_best_only=True)\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss', \n",
        "        mode='min', \n",
        "        min_delta=0.0001, \n",
        "        patience=10)\n",
        "\n",
        "    callbacks = [\n",
        "                 CustomMetrics(\n",
        "                     val_texts, \n",
        "                     val_labels, \n",
        "                     \"model_use_high_f1_score.h5\"\n",
        "                     ), \n",
        "                 early_stopping, \n",
        "                 checkpoint]\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(train_texts,\n",
        "                        train_labels,\n",
        "                        epochs=epochs,\n",
        "                        callbacks=callbacks,\n",
        "                        validation_data=(val_texts, val_labels),\n",
        "                        verbose=1,  \n",
        "                        batch_size=batch_size)\n",
        "\n",
        "    # Print results.\n",
        "    history = history.history\n",
        "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "            acc=history['val_accuracy'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "    #\n",
        "    #  Fine Tuning the model\n",
        "    #\n",
        "    model = build_use_model(train_use_layer = True)\n",
        "    optimizer = Adam(lr = 0.00001)\n",
        "    model.compile(optimizer, loss='binary_crossentropy', metrics=metrics)  \n",
        "\n",
        "    model.load_weights(\"model_use_low_val_loss.h5\")\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        'model_use_fine_tuned_low_val_loss.h5', \n",
        "        monitor='val_loss', \n",
        "        mode='min', \n",
        "        modelsave_best_only=True)\n",
        "\n",
        "    callbacks = [CustomMetrics(\n",
        "                     val_texts, \n",
        "                     val_labels, \n",
        "                     \"model_use_fine_tuned_high_f1_score.h5\"\n",
        "                     ), \n",
        "                 early_stopping, \n",
        "                 checkpoint]\n",
        "    history = model.fit(train_texts,\n",
        "                    train_labels,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=callbacks,\n",
        "                    validation_data=(val_texts, val_labels),\n",
        "                    verbose=1,  \n",
        "                    batch_size=batch_size)\n",
        "    \n",
        "    # Print results.\n",
        "    history = history.history\n",
        "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "            acc=history['val_accuracy'][-1], loss=history['val_loss'][-1]))\n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8Ds8zbiIX05",
        "colab_type": "text"
      },
      "source": [
        "# Plot Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QJ2EucDIEjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_data(data):\n",
        "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
        "    plot_frequency_distribution_of_ngrams(train_texts)\n",
        "    plot_sample_length_distribution(train_texts)\n",
        "    plot_class_distribution(train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0QDvyff8JaN",
        "colab_type": "text"
      },
      "source": [
        "# Invoke Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4VLRQJjO6SR",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t_la1fFO89u",
        "colab_type": "code",
        "outputId": "3a7f453a-ff88-4c00-c653-1228995b3a67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data = load_disaster_dataset()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Train (6090,)\n",
            "Shape of Validation  (1523,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8PMWQKY95Bc",
        "colab_type": "text"
      },
      "source": [
        "## Invoke N Gram Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoDmSYlx8MP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_ngram_model(data, epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6npWbAM-aWw",
        "colab_type": "text"
      },
      "source": [
        "## Invoke Sequence Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfR5EO4t--bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_sequence_model(data, epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0gYIVNkAMHN",
        "colab_type": "text"
      },
      "source": [
        "## Invoke FIne Tuned Sequence Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMWBz5bZ_T9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot_data(data)\n",
        "# train_fine_tuned_sequence_model(cleaned_data, \"/content\", epochs = 20)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo2vXB5WVoVf",
        "colab_type": "text"
      },
      "source": [
        "## Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XitAZRLNVsfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaned_data = clean_data(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ARlaVDmQhQH",
        "colab_type": "text"
      },
      "source": [
        "## Cleaned Data Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7BRJBTMAURu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot_data(cleaned_data)\n",
        "# train_fine_tuned_sequence_model(cleaned_data, \"/content\", epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckcUl6HESuIQ",
        "colab_type": "text"
      },
      "source": [
        "## Invoke Smaller Fine Tuned Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "typwHlPhSy05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaned_data = clean_data(data)\n",
        "# plot_data(cleaned_data)\n",
        "# train_smaller_fine_tuned_sequence_model(data, \"/content\", epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB5lWB3nUQXV",
        "colab_type": "text"
      },
      "source": [
        "## Smaller Fine tuning on cleaned Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U6hgK_jTDZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaned_data = clean_data(data)\n",
        "# plot_data(cleaned_data)\n",
        "# train_smaller_fine_tuned_sequence_model(data, \"/content\", epochs = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aEerUQ1aiPf",
        "colab_type": "text"
      },
      "source": [
        "## USE Model Training invocation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP-sVNBwUX9Y",
        "colab_type": "code",
        "outputId": "57a20670-abb8-4c16-844b-26ecf51040f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# ((x_train, y_train), (_,_)) = data\n",
        "# print(y_train.values)\n",
        "# x_train_c = tf.convert_to_tensor(x_train, dtype=tf.string)\n",
        "# print(x_train_c)\n",
        "# build_use_model(True)\n",
        "model = train_use_model(data, epochs=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "USE_embedding (KerasLayer)   {'outputs': (None, 512)}  147354880 \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 147,556,353\n",
            "Trainable params: 199,425\n",
            "Non-trainable params: 147,356,928\n",
            "_________________________________________________________________\n",
            "Train on 6090 samples, validate on 1523 samples\n",
            "Epoch 1/1000\n",
            "6080/6090 [============================>.] - ETA: 0s - loss: 1.4195 - accuracy: 0.5882 - recall_3: 0.5917 - precision_3: 0.5154 - val_f1_score:  0.45833333333333326\n",
            "6090/6090 [==============================] - 47s 8ms/sample - loss: 1.4199 - accuracy: 0.5882 - recall_3: 0.5919 - precision_3: 0.5154 - val_loss: 1.2653 - val_accuracy: 0.6756 - val_recall_3: 0.3115 - val_precision_3: 0.8672\n",
            "Epoch 2/1000\n",
            "6090/6090 [==============================] - 10s 2ms/sample - loss: 1.2788 - accuracy: 0.6680 - recall_3: 0.6496 - precision_3: 0.6032 - val_loss: 1.2490 - val_accuracy: 0.5469 - val_recall_3: 0.9508 - val_precision_3: 0.4927\n",
            "Epoch 3/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 1.2000 - accuracy: 0.7120 - recall_3: 0.6881 - precision_3: 0.6548 - val_loss: 1.2067 - val_accuracy: 0.6349 - val_recall_3: 0.9076 - val_precision_3: 0.5521\n",
            "Epoch 4/1000\n",
            "6090/6090 [==============================] - 20s 3ms/sample - loss: 1.1421 - accuracy: 0.7491 - recall_3: 0.7112 - precision_3: 0.7041 - val_loss: 1.1397 - val_accuracy: 0.7413 - val_recall_3: 0.8316 - val_precision_3: 0.6651\n",
            "Epoch 5/1000\n",
            "6090/6090 [==============================] - 10s 2ms/sample - loss: 1.1292 - accuracy: 0.7557 - recall_3: 0.7096 - precision_3: 0.7157 - val_loss: 1.0671 - val_accuracy: 0.7925 - val_recall_3: 0.7690 - val_precision_3: 0.7622\n",
            "Epoch 6/1000\n",
            "6080/6090 [============================>.] - ETA: 0s - loss: 1.1203 - accuracy: 0.7637 - recall_3: 0.7146 - precision_3: 0.7272 - val_f1_score:  0.7707390648567121\n",
            "6090/6090 [==============================] - 31s 5ms/sample - loss: 1.1199 - accuracy: 0.7639 - recall_3: 0.7146 - precision_3: 0.7275 - val_loss: 1.0248 - val_accuracy: 0.8004 - val_recall_3: 0.7615 - val_precision_3: 0.7802\n",
            "Epoch 7/1000\n",
            "6090/6090 [==============================] - 17s 3ms/sample - loss: 1.1057 - accuracy: 0.7601 - recall_3: 0.7073 - precision_3: 0.7243 - val_loss: 1.0048 - val_accuracy: 0.8037 - val_recall_3: 0.7422 - val_precision_3: 0.7981\n",
            "Epoch 8/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 1.0966 - accuracy: 0.7663 - recall_3: 0.7027 - precision_3: 0.7376 - val_loss: 0.9971 - val_accuracy: 0.8063 - val_recall_3: 0.7377 - val_precision_3: 0.8062\n",
            "Epoch 9/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 1.0639 - accuracy: 0.7816 - recall_3: 0.7238 - precision_3: 0.7546 - val_loss: 0.9930 - val_accuracy: 0.8050 - val_recall_3: 0.7288 - val_precision_3: 0.8096\n",
            "Epoch 10/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 1.0588 - accuracy: 0.7782 - recall_3: 0.7131 - precision_3: 0.7540 - val_loss: 0.9889 - val_accuracy: 0.8017 - val_recall_3: 0.7198 - val_precision_3: 0.8090\n",
            "Epoch 11/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 1.0553 - accuracy: 0.7808 - recall_3: 0.7085 - precision_3: 0.7615 - val_loss: 0.9830 - val_accuracy: 0.8102 - val_recall_3: 0.7213 - val_precision_3: 0.8259\n",
            "Epoch 12/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 1.0318 - accuracy: 0.7813 - recall_3: 0.7058 - precision_3: 0.7639 - val_loss: 0.9773 - val_accuracy: 0.8109 - val_recall_3: 0.7243 - val_precision_3: 0.8251\n",
            "Epoch 13/1000\n",
            "6090/6090 [==============================] - 17s 3ms/sample - loss: 1.0404 - accuracy: 0.7842 - recall_3: 0.7135 - precision_3: 0.7653 - val_loss: 0.9730 - val_accuracy: 0.8122 - val_recall_3: 0.7258 - val_precision_3: 0.8268\n",
            "Epoch 14/1000\n",
            "6090/6090 [==============================] - 16s 3ms/sample - loss: 1.0244 - accuracy: 0.7903 - recall_3: 0.7235 - precision_3: 0.7712 - val_loss: 0.9679 - val_accuracy: 0.8089 - val_recall_3: 0.7228 - val_precision_3: 0.8220\n",
            "Epoch 15/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 1.0094 - accuracy: 0.7989 - recall_3: 0.7312 - precision_3: 0.7833 - val_loss: 0.9629 - val_accuracy: 0.8096 - val_recall_3: 0.7228 - val_precision_3: 0.8234\n",
            "Epoch 16/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.9988 - accuracy: 0.7944 - recall_3: 0.7227 - precision_3: 0.7797 - val_loss: 0.9583 - val_accuracy: 0.8089 - val_recall_3: 0.7228 - val_precision_3: 0.8220\n",
            "Epoch 17/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.9855 - accuracy: 0.7951 - recall_3: 0.7188 - precision_3: 0.7833 - val_loss: 0.9542 - val_accuracy: 0.8076 - val_recall_3: 0.7228 - val_precision_3: 0.8193\n",
            "Epoch 18/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.9725 - accuracy: 0.8048 - recall_3: 0.7323 - precision_3: 0.7943 - val_loss: 0.9490 - val_accuracy: 0.8089 - val_recall_3: 0.7243 - val_precision_3: 0.8209\n",
            "Epoch 19/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.9837 - accuracy: 0.8003 - recall_3: 0.7292 - precision_3: 0.7874 - val_loss: 0.9443 - val_accuracy: 0.8096 - val_recall_3: 0.7243 - val_precision_3: 0.8223\n",
            "Epoch 20/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.9744 - accuracy: 0.8033 - recall_3: 0.7304 - precision_3: 0.7926 - val_loss: 0.9400 - val_accuracy: 0.8096 - val_recall_3: 0.7183 - val_precision_3: 0.8268\n",
            "Epoch 21/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.9711 - accuracy: 0.7957 - recall_3: 0.7285 - precision_3: 0.7788 - val_loss: 0.9354 - val_accuracy: 0.8109 - val_recall_3: 0.7198 - val_precision_3: 0.8285\n",
            "Epoch 22/1000\n",
            "6090/6090 [==============================] - 17s 3ms/sample - loss: 0.9505 - accuracy: 0.8102 - recall_3: 0.7373 - precision_3: 0.8021 - val_loss: 0.9312 - val_accuracy: 0.8148 - val_recall_3: 0.7228 - val_precision_3: 0.8348\n",
            "Epoch 23/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.9401 - accuracy: 0.8113 - recall_3: 0.7304 - precision_3: 0.8091 - val_loss: 0.9277 - val_accuracy: 0.8148 - val_recall_3: 0.7198 - val_precision_3: 0.8371\n",
            "Epoch 24/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.9368 - accuracy: 0.8099 - recall_3: 0.7358 - precision_3: 0.8024 - val_loss: 0.9232 - val_accuracy: 0.8194 - val_recall_3: 0.7228 - val_precision_3: 0.8449\n",
            "Epoch 25/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.9326 - accuracy: 0.8084 - recall_3: 0.7335 - precision_3: 0.8009 - val_loss: 0.9190 - val_accuracy: 0.8194 - val_recall_3: 0.7228 - val_precision_3: 0.8449\n",
            "Epoch 26/1000\n",
            "6080/6090 [============================>.] - ETA: 0s - loss: 0.9308 - accuracy: 0.8090 - recall_3: 0.7327 - precision_3: 0.8029 - val_f1_score:  0.7741420590582603\n",
            "6090/6090 [==============================] - 29s 5ms/sample - loss: 0.9302 - accuracy: 0.8094 - recall_3: 0.7331 - precision_3: 0.8032 - val_loss: 0.9148 - val_accuracy: 0.8142 - val_recall_3: 0.7228 - val_precision_3: 0.8333\n",
            "Epoch 27/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.9166 - accuracy: 0.8167 - recall_3: 0.7442 - precision_3: 0.8110 - val_loss: 0.9096 - val_accuracy: 0.8201 - val_recall_3: 0.7243 - val_precision_3: 0.8452\n",
            "Epoch 28/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.9160 - accuracy: 0.8067 - recall_3: 0.7235 - precision_3: 0.8042 - val_loss: 0.9062 - val_accuracy: 0.8214 - val_recall_3: 0.7228 - val_precision_3: 0.8494\n",
            "Epoch 29/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.8970 - accuracy: 0.8205 - recall_3: 0.7388 - precision_3: 0.8227 - val_loss: 0.9011 - val_accuracy: 0.8188 - val_recall_3: 0.7273 - val_precision_3: 0.8399\n",
            "Epoch 30/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.8944 - accuracy: 0.8149 - recall_3: 0.7335 - precision_3: 0.8146 - val_loss: 0.8963 - val_accuracy: 0.8194 - val_recall_3: 0.7243 - val_precision_3: 0.8438\n",
            "Epoch 31/1000\n",
            "6080/6090 [============================>.] - ETA: 0s - loss: 0.8856 - accuracy: 0.8219 - recall_3: 0.7477 - precision_3: 0.8193 - val_f1_score:  0.7776892430278884\n",
            "6090/6090 [==============================] - 30s 5ms/sample - loss: 0.8854 - accuracy: 0.8220 - recall_3: 0.7477 - precision_3: 0.8196 - val_loss: 0.8921 - val_accuracy: 0.8168 - val_recall_3: 0.7273 - val_precision_3: 0.8356\n",
            "Epoch 32/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.8912 - accuracy: 0.8128 - recall_3: 0.7273 - precision_3: 0.8144 - val_loss: 0.8881 - val_accuracy: 0.8188 - val_recall_3: 0.7288 - val_precision_3: 0.8388\n",
            "Epoch 33/1000\n",
            "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.8758 - accuracy: 0.8184 - recall_3: 0.7465 - precision_3: 0.8128 - val_loss: 0.8854 - val_accuracy: 0.8194 - val_recall_3: 0.7303 - val_precision_3: 0.8390\n",
            "Epoch 34/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.8644 - accuracy: 0.8300 - recall_3: 0.7512 - precision_3: 0.8343 - val_loss: 0.8813 - val_accuracy: 0.8162 - val_recall_3: 0.7228 - val_precision_3: 0.8377\n",
            "Epoch 35/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.8712 - accuracy: 0.8238 - recall_3: 0.7454 - precision_3: 0.8250 - val_loss: 0.8754 - val_accuracy: 0.8201 - val_recall_3: 0.7273 - val_precision_3: 0.8428\n",
            "Epoch 36/1000\n",
            "6080/6090 [============================>.] - ETA: 0s - loss: 0.8601 - accuracy: 0.8209 - recall_3: 0.7478 - precision_3: 0.8173 - val_f1_score:  0.7830264211369095\n",
            "6090/6090 [==============================] - 28s 5ms/sample - loss: 0.8610 - accuracy: 0.8202 - recall_3: 0.7473 - precision_3: 0.8160 - val_loss: 0.8708 - val_accuracy: 0.8221 - val_recall_3: 0.7288 - val_precision_3: 0.8460\n",
            "Epoch 37/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.8495 - accuracy: 0.8266 - recall_3: 0.7477 - precision_3: 0.8294 - val_loss: 0.8678 - val_accuracy: 0.8181 - val_recall_3: 0.7273 - val_precision_3: 0.8385\n",
            "Epoch 38/1000\n",
            "6090/6090 [==============================] - 15s 2ms/sample - loss: 0.8371 - accuracy: 0.8282 - recall_3: 0.7508 - precision_3: 0.8306 - val_loss: 0.8642 - val_accuracy: 0.8175 - val_recall_3: 0.7213 - val_precision_3: 0.8417\n",
            "Epoch 39/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.8339 - accuracy: 0.8325 - recall_3: 0.7562 - precision_3: 0.8359 - val_loss: 0.8595 - val_accuracy: 0.8194 - val_recall_3: 0.7213 - val_precision_3: 0.8462\n",
            "Epoch 40/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.8305 - accuracy: 0.8304 - recall_3: 0.7442 - precision_3: 0.8402 - val_loss: 0.8551 - val_accuracy: 0.8207 - val_recall_3: 0.7258 - val_precision_3: 0.8455\n",
            "Epoch 41/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.8285 - accuracy: 0.8241 - recall_3: 0.7392 - precision_3: 0.8302 - val_loss: 0.8501 - val_accuracy: 0.8207 - val_recall_3: 0.7303 - val_precision_3: 0.8419\n",
            "Epoch 42/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.8243 - accuracy: 0.8304 - recall_3: 0.7527 - precision_3: 0.8338 - val_loss: 0.8469 - val_accuracy: 0.8194 - val_recall_3: 0.7303 - val_precision_3: 0.8390\n",
            "Epoch 43/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.8174 - accuracy: 0.8304 - recall_3: 0.7515 - precision_3: 0.8347 - val_loss: 0.8417 - val_accuracy: 0.8181 - val_recall_3: 0.7317 - val_precision_3: 0.8350\n",
            "Epoch 44/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.8125 - accuracy: 0.8315 - recall_3: 0.7565 - precision_3: 0.8335 - val_loss: 0.8385 - val_accuracy: 0.8181 - val_recall_3: 0.7288 - val_precision_3: 0.8373\n",
            "Epoch 45/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.8087 - accuracy: 0.8323 - recall_3: 0.7585 - precision_3: 0.8338 - val_loss: 0.8344 - val_accuracy: 0.8201 - val_recall_3: 0.7317 - val_precision_3: 0.8393\n",
            "Epoch 46/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.8007 - accuracy: 0.8338 - recall_3: 0.7512 - precision_3: 0.8425 - val_loss: 0.8301 - val_accuracy: 0.8194 - val_recall_3: 0.7347 - val_precision_3: 0.8356\n",
            "Epoch 47/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.7948 - accuracy: 0.8319 - recall_3: 0.7577 - precision_3: 0.8333 - val_loss: 0.8259 - val_accuracy: 0.8207 - val_recall_3: 0.7303 - val_precision_3: 0.8419\n",
            "Epoch 48/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.7898 - accuracy: 0.8322 - recall_3: 0.7604 - precision_3: 0.8321 - val_loss: 0.8219 - val_accuracy: 0.8234 - val_recall_3: 0.7288 - val_precision_3: 0.8490\n",
            "Epoch 49/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.7792 - accuracy: 0.8397 - recall_3: 0.7577 - precision_3: 0.8506 - val_loss: 0.8183 - val_accuracy: 0.8234 - val_recall_3: 0.7303 - val_precision_3: 0.8478\n",
            "Epoch 50/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.7720 - accuracy: 0.8366 - recall_3: 0.7515 - precision_3: 0.8485 - val_loss: 0.8141 - val_accuracy: 0.8227 - val_recall_3: 0.7303 - val_precision_3: 0.8463\n",
            "Epoch 51/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.7684 - accuracy: 0.8379 - recall_3: 0.7588 - precision_3: 0.8457 - val_loss: 0.8094 - val_accuracy: 0.8201 - val_recall_3: 0.7243 - val_precision_3: 0.8452\n",
            "Epoch 52/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.7613 - accuracy: 0.8399 - recall_3: 0.7635 - precision_3: 0.8465 - val_loss: 0.8048 - val_accuracy: 0.8234 - val_recall_3: 0.7243 - val_precision_3: 0.8526\n",
            "Epoch 53/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.7573 - accuracy: 0.8402 - recall_3: 0.7562 - precision_3: 0.8529 - val_loss: 0.8024 - val_accuracy: 0.8234 - val_recall_3: 0.7303 - val_precision_3: 0.8478\n",
            "Epoch 54/1000\n",
            "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.7522 - accuracy: 0.8376 - recall_3: 0.7600 - precision_3: 0.8441 - val_loss: 0.7995 - val_accuracy: 0.8267 - val_recall_3: 0.7288 - val_precision_3: 0.8564\n",
            "Epoch 55/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.7552 - accuracy: 0.8384 - recall_3: 0.7635 - precision_3: 0.8432 - val_loss: 0.7963 - val_accuracy: 0.8221 - val_recall_3: 0.7288 - val_precision_3: 0.8460\n",
            "Epoch 56/1000\n",
            "6080/6090 [============================>.] - ETA: 0s - loss: 0.7408 - accuracy: 0.8418 - recall_3: 0.7663 - precision_3: 0.8481 - val_f1_score:  0.7896857373086221\n",
            "6090/6090 [==============================] - 33s 5ms/sample - loss: 0.7415 - accuracy: 0.8414 - recall_3: 0.7654 - precision_3: 0.8483 - val_loss: 0.7925 - val_accuracy: 0.8286 - val_recall_3: 0.7303 - val_precision_3: 0.8596\n",
            "Epoch 57/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.7421 - accuracy: 0.8447 - recall_3: 0.7719 - precision_3: 0.8504 - val_loss: 0.7899 - val_accuracy: 0.8260 - val_recall_3: 0.7288 - val_precision_3: 0.8549\n",
            "Epoch 58/1000\n",
            "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.7285 - accuracy: 0.8468 - recall_3: 0.7669 - precision_3: 0.8591 - val_loss: 0.7863 - val_accuracy: 0.8253 - val_recall_3: 0.7288 - val_precision_3: 0.8534\n",
            "Epoch 59/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.7220 - accuracy: 0.8506 - recall_3: 0.7738 - precision_3: 0.8620 - val_loss: 0.7829 - val_accuracy: 0.8260 - val_recall_3: 0.7288 - val_precision_3: 0.8549\n",
            "Epoch 60/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.7237 - accuracy: 0.8424 - recall_3: 0.7662 - precision_3: 0.8498 - val_loss: 0.7815 - val_accuracy: 0.8299 - val_recall_3: 0.7332 - val_precision_3: 0.8601\n",
            "Epoch 61/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.7085 - accuracy: 0.8481 - recall_3: 0.7685 - precision_3: 0.8608 - val_loss: 0.7782 - val_accuracy: 0.8286 - val_recall_3: 0.7243 - val_precision_3: 0.8648\n",
            "Epoch 62/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.7121 - accuracy: 0.8443 - recall_3: 0.7642 - precision_3: 0.8557 - val_loss: 0.7745 - val_accuracy: 0.8253 - val_recall_3: 0.7303 - val_precision_3: 0.8522\n",
            "Epoch 63/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.7177 - accuracy: 0.8376 - recall_3: 0.7562 - precision_3: 0.8470 - val_loss: 0.7703 - val_accuracy: 0.8280 - val_recall_3: 0.7317 - val_precision_3: 0.8569\n",
            "Epoch 64/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.7017 - accuracy: 0.8504 - recall_3: 0.7669 - precision_3: 0.8673 - val_loss: 0.7677 - val_accuracy: 0.8280 - val_recall_3: 0.7303 - val_precision_3: 0.8581\n",
            "Epoch 65/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.6963 - accuracy: 0.8481 - recall_3: 0.7715 - precision_3: 0.8584 - val_loss: 0.7647 - val_accuracy: 0.8299 - val_recall_3: 0.7303 - val_precision_3: 0.8627\n",
            "Epoch 66/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.6958 - accuracy: 0.8502 - recall_3: 0.7781 - precision_3: 0.8579 - val_loss: 0.7609 - val_accuracy: 0.8267 - val_recall_3: 0.7258 - val_precision_3: 0.8589\n",
            "Epoch 67/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.6850 - accuracy: 0.8548 - recall_3: 0.7792 - precision_3: 0.8673 - val_loss: 0.7570 - val_accuracy: 0.8306 - val_recall_3: 0.7377 - val_precision_3: 0.8579\n",
            "Epoch 68/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.6907 - accuracy: 0.8498 - recall_3: 0.7746 - precision_3: 0.8596 - val_loss: 0.7545 - val_accuracy: 0.8293 - val_recall_3: 0.7392 - val_precision_3: 0.8537\n",
            "Epoch 69/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.6845 - accuracy: 0.8481 - recall_3: 0.7738 - precision_3: 0.8565 - val_loss: 0.7510 - val_accuracy: 0.8273 - val_recall_3: 0.7362 - val_precision_3: 0.8517\n",
            "Epoch 70/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.6836 - accuracy: 0.8491 - recall_3: 0.7696 - precision_3: 0.8621 - val_loss: 0.7471 - val_accuracy: 0.8234 - val_recall_3: 0.7303 - val_precision_3: 0.8478\n",
            "Epoch 71/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.6779 - accuracy: 0.8465 - recall_3: 0.7762 - precision_3: 0.8511 - val_loss: 0.7422 - val_accuracy: 0.8260 - val_recall_3: 0.7317 - val_precision_3: 0.8524\n",
            "Epoch 72/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.6609 - accuracy: 0.8624 - recall_3: 0.7885 - precision_3: 0.8768 - val_loss: 0.7398 - val_accuracy: 0.8267 - val_recall_3: 0.7317 - val_precision_3: 0.8539\n",
            "Epoch 73/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.6584 - accuracy: 0.8535 - recall_3: 0.7796 - precision_3: 0.8640 - val_loss: 0.7389 - val_accuracy: 0.8260 - val_recall_3: 0.7332 - val_precision_3: 0.8512\n",
            "Epoch 74/1000\n",
            "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.6602 - accuracy: 0.8511 - recall_3: 0.7769 - precision_3: 0.8607 - val_loss: 0.7357 - val_accuracy: 0.8299 - val_recall_3: 0.7347 - val_precision_3: 0.8589\n",
            "Epoch 75/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.6588 - accuracy: 0.8517 - recall_3: 0.7815 - precision_3: 0.8585 - val_loss: 0.7332 - val_accuracy: 0.8253 - val_recall_3: 0.7258 - val_precision_3: 0.8559\n",
            "Epoch 76/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.6441 - accuracy: 0.8558 - recall_3: 0.7808 - precision_3: 0.8683 - val_loss: 0.7313 - val_accuracy: 0.8240 - val_recall_3: 0.7288 - val_precision_3: 0.8504\n",
            "Epoch 77/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.6455 - accuracy: 0.8567 - recall_3: 0.7846 - precision_3: 0.8670 - val_loss: 0.7296 - val_accuracy: 0.8273 - val_recall_3: 0.7347 - val_precision_3: 0.8529\n",
            "Epoch 78/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.6367 - accuracy: 0.8589 - recall_3: 0.7869 - precision_3: 0.8703 - val_loss: 0.7267 - val_accuracy: 0.8221 - val_recall_3: 0.7303 - val_precision_3: 0.8448\n",
            "Epoch 79/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.6317 - accuracy: 0.8599 - recall_3: 0.7942 - precision_3: 0.8666 - val_loss: 0.7246 - val_accuracy: 0.8234 - val_recall_3: 0.7258 - val_precision_3: 0.8514\n",
            "Epoch 80/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.6322 - accuracy: 0.8601 - recall_3: 0.7862 - precision_3: 0.8735 - val_loss: 0.7208 - val_accuracy: 0.8207 - val_recall_3: 0.7303 - val_precision_3: 0.8419\n",
            "Epoch 81/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.6215 - accuracy: 0.8639 - recall_3: 0.7969 - precision_3: 0.8732 - val_loss: 0.7186 - val_accuracy: 0.8227 - val_recall_3: 0.7332 - val_precision_3: 0.8439\n",
            "Epoch 82/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.6204 - accuracy: 0.8598 - recall_3: 0.7888 - precision_3: 0.8705 - val_loss: 0.7189 - val_accuracy: 0.8234 - val_recall_3: 0.7332 - val_precision_3: 0.8454\n",
            "Epoch 83/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.6209 - accuracy: 0.8573 - recall_3: 0.7942 - precision_3: 0.8608 - val_loss: 0.7137 - val_accuracy: 0.8234 - val_recall_3: 0.7332 - val_precision_3: 0.8454\n",
            "Epoch 84/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.5998 - accuracy: 0.8681 - recall_3: 0.8038 - precision_3: 0.8770 - val_loss: 0.7117 - val_accuracy: 0.8227 - val_recall_3: 0.7362 - val_precision_3: 0.8416\n",
            "Epoch 85/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.6095 - accuracy: 0.8677 - recall_3: 0.8050 - precision_3: 0.8750 - val_loss: 0.7087 - val_accuracy: 0.8221 - val_recall_3: 0.7303 - val_precision_3: 0.8448\n",
            "Epoch 86/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.6097 - accuracy: 0.8609 - recall_3: 0.8004 - precision_3: 0.8638 - val_loss: 0.7063 - val_accuracy: 0.8214 - val_recall_3: 0.7273 - val_precision_3: 0.8458\n",
            "Epoch 87/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.5959 - accuracy: 0.8677 - recall_3: 0.7977 - precision_3: 0.8811 - val_loss: 0.7048 - val_accuracy: 0.8247 - val_recall_3: 0.7317 - val_precision_3: 0.8495\n",
            "Epoch 88/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.5878 - accuracy: 0.8704 - recall_3: 0.8119 - precision_3: 0.8756 - val_loss: 0.7029 - val_accuracy: 0.8253 - val_recall_3: 0.7303 - val_precision_3: 0.8522\n",
            "Epoch 89/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.5884 - accuracy: 0.8642 - recall_3: 0.8031 - precision_3: 0.8689 - val_loss: 0.7011 - val_accuracy: 0.8247 - val_recall_3: 0.7332 - val_precision_3: 0.8483\n",
            "Epoch 90/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.5852 - accuracy: 0.8678 - recall_3: 0.8035 - precision_3: 0.8766 - val_loss: 0.6997 - val_accuracy: 0.8247 - val_recall_3: 0.7332 - val_precision_3: 0.8483\n",
            "Epoch 91/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.5806 - accuracy: 0.8681 - recall_3: 0.8000 - precision_3: 0.8802 - val_loss: 0.6976 - val_accuracy: 0.8214 - val_recall_3: 0.7317 - val_precision_3: 0.8422\n",
            "Epoch 92/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.5735 - accuracy: 0.8660 - recall_3: 0.7988 - precision_3: 0.8764 - val_loss: 0.6970 - val_accuracy: 0.8260 - val_recall_3: 0.7332 - val_precision_3: 0.8512\n",
            "Epoch 93/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.5736 - accuracy: 0.8734 - recall_3: 0.8131 - precision_3: 0.8812 - val_loss: 0.6954 - val_accuracy: 0.8286 - val_recall_3: 0.7362 - val_precision_3: 0.8547\n",
            "Epoch 94/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.5800 - accuracy: 0.8663 - recall_3: 0.8092 - precision_3: 0.8687 - val_loss: 0.6927 - val_accuracy: 0.8207 - val_recall_3: 0.7332 - val_precision_3: 0.8396\n",
            "Epoch 95/1000\n",
            "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.5669 - accuracy: 0.8729 - recall_3: 0.8169 - precision_3: 0.8770 - val_loss: 0.6910 - val_accuracy: 0.8260 - val_recall_3: 0.7347 - val_precision_3: 0.8500\n",
            "Epoch 96/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.5627 - accuracy: 0.8714 - recall_3: 0.8142 - precision_3: 0.8759 - val_loss: 0.6880 - val_accuracy: 0.8267 - val_recall_3: 0.7347 - val_precision_3: 0.8515\n",
            "Epoch 97/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.5628 - accuracy: 0.8727 - recall_3: 0.8138 - precision_3: 0.8791 - val_loss: 0.6871 - val_accuracy: 0.8247 - val_recall_3: 0.7317 - val_precision_3: 0.8495\n",
            "Epoch 98/1000\n",
            "6090/6090 [==============================] - 15s 2ms/sample - loss: 0.5550 - accuracy: 0.8708 - recall_3: 0.8169 - precision_3: 0.8723 - val_loss: 0.6883 - val_accuracy: 0.8260 - val_recall_3: 0.7362 - val_precision_3: 0.8488\n",
            "Epoch 99/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.5402 - accuracy: 0.8795 - recall_3: 0.8158 - precision_3: 0.8927 - val_loss: 0.6877 - val_accuracy: 0.8253 - val_recall_3: 0.7377 - val_precision_3: 0.8462\n",
            "Epoch 100/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.5400 - accuracy: 0.8811 - recall_3: 0.8208 - precision_3: 0.8921 - val_loss: 0.6862 - val_accuracy: 0.8214 - val_recall_3: 0.7317 - val_precision_3: 0.8422\n",
            "Epoch 101/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.5436 - accuracy: 0.8749 - recall_3: 0.8092 - precision_3: 0.8878 - val_loss: 0.6856 - val_accuracy: 0.8168 - val_recall_3: 0.7317 - val_precision_3: 0.8322\n",
            "Epoch 102/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.5377 - accuracy: 0.8760 - recall_3: 0.8258 - precision_3: 0.8767 - val_loss: 0.6826 - val_accuracy: 0.8207 - val_recall_3: 0.7347 - val_precision_3: 0.8384\n",
            "Epoch 103/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.5329 - accuracy: 0.8770 - recall_3: 0.8185 - precision_3: 0.8848 - val_loss: 0.6799 - val_accuracy: 0.8201 - val_recall_3: 0.7288 - val_precision_3: 0.8417\n",
            "Epoch 104/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.5316 - accuracy: 0.8772 - recall_3: 0.8242 - precision_3: 0.8804 - val_loss: 0.6770 - val_accuracy: 0.8207 - val_recall_3: 0.7332 - val_precision_3: 0.8396\n",
            "Epoch 105/1000\n",
            "6090/6090 [==============================] - 15s 3ms/sample - loss: 0.5222 - accuracy: 0.8828 - recall_3: 0.8308 - precision_3: 0.8874 - val_loss: 0.6782 - val_accuracy: 0.8201 - val_recall_3: 0.7288 - val_precision_3: 0.8417\n",
            "Epoch 106/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.5234 - accuracy: 0.8836 - recall_3: 0.8258 - precision_3: 0.8935 - val_loss: 0.6789 - val_accuracy: 0.8155 - val_recall_3: 0.7288 - val_precision_3: 0.8316\n",
            "Epoch 107/1000\n",
            "6090/6090 [==============================] - 17s 3ms/sample - loss: 0.5164 - accuracy: 0.8860 - recall_3: 0.8358 - precision_3: 0.8906 - val_loss: 0.6756 - val_accuracy: 0.8227 - val_recall_3: 0.7392 - val_precision_3: 0.8393\n",
            "Epoch 108/1000\n",
            "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.5090 - accuracy: 0.8885 - recall_3: 0.8465 - precision_3: 0.8871 - val_loss: 0.6779 - val_accuracy: 0.8175 - val_recall_3: 0.7407 - val_precision_3: 0.8270\n",
            "Epoch 109/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.5097 - accuracy: 0.8837 - recall_3: 0.8315 - precision_3: 0.8890 - val_loss: 0.6773 - val_accuracy: 0.8207 - val_recall_3: 0.7392 - val_precision_3: 0.8350\n",
            "Epoch 110/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.5081 - accuracy: 0.8847 - recall_3: 0.8300 - precision_3: 0.8925 - val_loss: 0.6720 - val_accuracy: 0.8240 - val_recall_3: 0.7422 - val_precision_3: 0.8398\n",
            "Epoch 111/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.5045 - accuracy: 0.8852 - recall_3: 0.8319 - precision_3: 0.8920 - val_loss: 0.6719 - val_accuracy: 0.8234 - val_recall_3: 0.7347 - val_precision_3: 0.8442\n",
            "Epoch 112/1000\n",
            "6090/6090 [==============================] - 17s 3ms/sample - loss: 0.5070 - accuracy: 0.8810 - recall_3: 0.8242 - precision_3: 0.8888 - val_loss: 0.6712 - val_accuracy: 0.8234 - val_recall_3: 0.7377 - val_precision_3: 0.8418\n",
            "Epoch 113/1000\n",
            "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.5005 - accuracy: 0.8816 - recall_3: 0.8254 - precision_3: 0.8893 - val_loss: 0.6663 - val_accuracy: 0.8234 - val_recall_3: 0.7377 - val_precision_3: 0.8418\n",
            "Epoch 114/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.4926 - accuracy: 0.8867 - recall_3: 0.8354 - precision_3: 0.8924 - val_loss: 0.6659 - val_accuracy: 0.8260 - val_recall_3: 0.7392 - val_precision_3: 0.8464\n",
            "Epoch 115/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.4977 - accuracy: 0.8847 - recall_3: 0.8369 - precision_3: 0.8867 - val_loss: 0.6649 - val_accuracy: 0.8227 - val_recall_3: 0.7362 - val_precision_3: 0.8416\n",
            "Epoch 116/1000\n",
            "6090/6090 [==============================] - 13s 2ms/sample - loss: 0.4867 - accuracy: 0.8878 - recall_3: 0.8419 - precision_3: 0.8895 - val_loss: 0.6627 - val_accuracy: 0.8181 - val_recall_3: 0.7377 - val_precision_3: 0.8305\n",
            "Epoch 117/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.4947 - accuracy: 0.8831 - recall_3: 0.8319 - precision_3: 0.8872 - val_loss: 0.6645 - val_accuracy: 0.8168 - val_recall_3: 0.7362 - val_precision_3: 0.8289\n",
            "Epoch 118/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.4793 - accuracy: 0.8898 - recall_3: 0.8435 - precision_3: 0.8926 - val_loss: 0.6622 - val_accuracy: 0.8201 - val_recall_3: 0.7362 - val_precision_3: 0.8359\n",
            "Epoch 119/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.4806 - accuracy: 0.8936 - recall_3: 0.8492 - precision_3: 0.8961 - val_loss: 0.6616 - val_accuracy: 0.8221 - val_recall_3: 0.7377 - val_precision_3: 0.8390\n",
            "Epoch 120/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.4633 - accuracy: 0.8977 - recall_3: 0.8535 - precision_3: 0.9017 - val_loss: 0.6662 - val_accuracy: 0.8188 - val_recall_3: 0.7288 - val_precision_3: 0.8388\n",
            "Epoch 121/1000\n",
            "6090/6090 [==============================] - 19s 3ms/sample - loss: 0.4819 - accuracy: 0.8913 - recall_3: 0.8431 - precision_3: 0.8962 - val_loss: 0.6652 - val_accuracy: 0.8207 - val_recall_3: 0.7347 - val_precision_3: 0.8384\n",
            "Epoch 122/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.4699 - accuracy: 0.8885 - recall_3: 0.8400 - precision_3: 0.8925 - val_loss: 0.6671 - val_accuracy: 0.8188 - val_recall_3: 0.7377 - val_precision_3: 0.8319\n",
            "Epoch 123/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.4687 - accuracy: 0.8888 - recall_3: 0.8492 - precision_3: 0.8857 - val_loss: 0.6666 - val_accuracy: 0.8201 - val_recall_3: 0.7377 - val_precision_3: 0.8347\n",
            "Epoch 124/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.4652 - accuracy: 0.8915 - recall_3: 0.8469 - precision_3: 0.8933 - val_loss: 0.6617 - val_accuracy: 0.8234 - val_recall_3: 0.7377 - val_precision_3: 0.8418\n",
            "Epoch 125/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.4547 - accuracy: 0.8970 - recall_3: 0.8554 - precision_3: 0.8986 - val_loss: 0.6625 - val_accuracy: 0.8227 - val_recall_3: 0.7377 - val_precision_3: 0.8404\n",
            "Epoch 126/1000\n",
            "6090/6090 [==============================] - 12s 2ms/sample - loss: 0.4586 - accuracy: 0.8990 - recall_3: 0.8588 - precision_3: 0.9000 - val_loss: 0.6626 - val_accuracy: 0.8214 - val_recall_3: 0.7347 - val_precision_3: 0.8399\n",
            "Epoch 127/1000\n",
            "6090/6090 [==============================] - 18s 3ms/sample - loss: 0.4506 - accuracy: 0.8974 - recall_3: 0.8527 - precision_3: 0.9016 - val_loss: 0.6631 - val_accuracy: 0.8188 - val_recall_3: 0.7303 - val_precision_3: 0.8376\n",
            "Epoch 128/1000\n",
            "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.4649 - accuracy: 0.8918 - recall_3: 0.8508 - precision_3: 0.8909 - val_loss: 0.6639 - val_accuracy: 0.8155 - val_recall_3: 0.7332 - val_precision_3: 0.8283\n",
            "Epoch 129/1000\n",
            "6090/6090 [==============================] - 11s 2ms/sample - loss: 0.4570 - accuracy: 0.8915 - recall_3: 0.8488 - precision_3: 0.8917 - val_loss: 0.6651 - val_accuracy: 0.8155 - val_recall_3: 0.7273 - val_precision_3: 0.8328\n",
            "Validation accuracy: 0.8154957294464111, loss: 0.6650785617897249\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "USE_embedding (KerasLayer)   {'outputs': (None, 512)}  147354880 \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 147,556,353\n",
            "Trainable params: 147,554,305\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n",
            "Train on 6090 samples, validate on 1523 samples\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_29:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_29:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_29:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_0/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_1/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_2/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_3/dense/kernel/part_29:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_0:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_1:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_2:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_3:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_4:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_5:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_6:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_7:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_8:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_9:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_10:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_11:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_12:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_13:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_14:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_15:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_16:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_17:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_18:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_19:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_20:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_21:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_22:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_23:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_24:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_25:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_26:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_27:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_28:0', 'EncoderDNN/DNN/ResidualHidden_3/AdjustDepth/projection/kernel/part_29:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6080/6090 [============================>.] - ETA: 0s - loss: 0.4491 - accuracy: 0.8982 - recall_3: 0.8597 - precision_3: 0.8974 - val_f1_score:  0.7789142407553108\n",
            "6090/6090 [==============================] - 147s 24ms/sample - loss: 0.4496 - accuracy: 0.8980 - recall_3: 0.8596 - precision_3: 0.8972 - val_loss: 0.6656 - val_accuracy: 0.8155 - val_recall_3: 0.7377 - val_precision_3: 0.8250\n",
            "Epoch 2/1000\n",
            "6090/6090 [==============================] - 45s 7ms/sample - loss: 0.4405 - accuracy: 0.9013 - recall_3: 0.8523 - precision_3: 0.9108 - val_loss: 0.6652 - val_accuracy: 0.8148 - val_recall_3: 0.7377 - val_precision_3: 0.8236\n",
            "Epoch 3/1000\n",
            "6090/6090 [==============================] - 46s 8ms/sample - loss: 0.4315 - accuracy: 0.9059 - recall_3: 0.8581 - precision_3: 0.9162 - val_loss: 0.6670 - val_accuracy: 0.8162 - val_recall_3: 0.7332 - val_precision_3: 0.8297\n",
            "Epoch 4/1000\n",
            "6090/6090 [==============================] - 44s 7ms/sample - loss: 0.4225 - accuracy: 0.9107 - recall_3: 0.8677 - precision_3: 0.9186 - val_loss: 0.6681 - val_accuracy: 0.8175 - val_recall_3: 0.7347 - val_precision_3: 0.8314\n",
            "Epoch 5/1000\n",
            "6090/6090 [==============================] - 44s 7ms/sample - loss: 0.4184 - accuracy: 0.9138 - recall_3: 0.8738 - precision_3: 0.9202 - val_loss: 0.6681 - val_accuracy: 0.8194 - val_recall_3: 0.7332 - val_precision_3: 0.8367\n",
            "Epoch 6/1000\n",
            "6080/6090 [============================>.] - ETA: 0s - loss: 0.4174 - accuracy: 0.9123 - recall_3: 0.8691 - precision_3: 0.9212 - val_f1_score:  0.7828843106180664\n",
            "6090/6090 [==============================] - 60s 10ms/sample - loss: 0.4172 - accuracy: 0.9125 - recall_3: 0.8692 - precision_3: 0.9213 - val_loss: 0.6715 - val_accuracy: 0.8201 - val_recall_3: 0.7362 - val_precision_3: 0.8359\n",
            "Epoch 7/1000\n",
            "6090/6090 [==============================] - 44s 7ms/sample - loss: 0.4102 - accuracy: 0.9166 - recall_3: 0.8831 - precision_3: 0.9184 - val_loss: 0.6711 - val_accuracy: 0.8214 - val_recall_3: 0.7392 - val_precision_3: 0.8364\n",
            "Epoch 8/1000\n",
            "6090/6090 [==============================] - 44s 7ms/sample - loss: 0.3879 - accuracy: 0.9278 - recall_3: 0.8950 - precision_3: 0.9330 - val_loss: 0.6721 - val_accuracy: 0.8221 - val_recall_3: 0.7407 - val_precision_3: 0.8367\n",
            "Epoch 9/1000\n",
            "6090/6090 [==============================] - 44s 7ms/sample - loss: 0.3980 - accuracy: 0.9159 - recall_3: 0.8765 - precision_3: 0.9227 - val_loss: 0.6767 - val_accuracy: 0.8221 - val_recall_3: 0.7437 - val_precision_3: 0.8344\n",
            "Epoch 10/1000\n",
            "6090/6090 [==============================] - 44s 7ms/sample - loss: 0.3865 - accuracy: 0.9238 - recall_3: 0.8896 - precision_3: 0.9289 - val_loss: 0.6776 - val_accuracy: 0.8234 - val_recall_3: 0.7481 - val_precision_3: 0.8339\n",
            "Epoch 11/1000\n",
            "6080/6090 [============================>.] - ETA: 0s - loss: 0.3761 - accuracy: 0.9306 - recall_3: 0.8971 - precision_3: 0.9376 - val_f1_score:  0.786833855799373\n",
            "6090/6090 [==============================] - 58s 10ms/sample - loss: 0.3760 - accuracy: 0.9305 - recall_3: 0.8969 - precision_3: 0.9377 - val_loss: 0.6801 - val_accuracy: 0.8214 - val_recall_3: 0.7481 - val_precision_3: 0.8298\n",
            "Epoch 12/1000\n",
            "6090/6090 [==============================] - 45s 7ms/sample - loss: 0.3760 - accuracy: 0.9305 - recall_3: 0.8962 - precision_3: 0.9384 - val_loss: 0.6828 - val_accuracy: 0.8240 - val_recall_3: 0.7481 - val_precision_3: 0.8353\n",
            "Validation accuracy: 0.8240315318107605, loss: 0.6827727520223206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2QZ7Di2mutb",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90hrKAQmZ87P",
        "colab_type": "code",
        "outputId": "8950ffba-f2b5-4b4c-ebb9-bacdd1ecf4e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "source": [
        "\n",
        "((x_train, y_train), (X_test,y_test)) = data\n",
        "train_texts = tf.convert_to_tensor(x_train.values, dtype=tf.string)\n",
        "train_labels = tf.convert_to_tensor(y_train.values, dtype=tf.bool)\n",
        "val_texts = tf.convert_to_tensor(X_test.values, dtype=tf.string)\n",
        "val_labels = tf.convert_to_tensor(y_test.values, dtype=tf.bool)\n",
        "\n",
        "\n",
        "model.load_weights('model_use_fine_tuned_high_f1_score.h5')\n",
        "predictions = model.predict(val_texts)\n",
        "\n",
        "def findClasses(predictions):\n",
        "    true_preds = []\n",
        "    a=1\n",
        "    b=0\n",
        "\n",
        "    for i in predictions:\n",
        "        if i >= 0.5:\n",
        "            true_preds.append(a)\n",
        "        else:\n",
        "            true_preds.append(b)\n",
        "    return true_preds\n",
        "\n",
        "classes = findClasses(predictions)\n",
        "print(classification_report(val_labels, classes, target_names=[\"Real\", \"Not Real\"]))\n",
        "\n",
        "\n",
        "test_data =  pd.read_csv(\"https://raw.githubusercontent.com/Konerusudhir/kaggle_datasets/master/nlp/disaster/test.csv\", error_bad_lines=False)\n",
        "\n",
        "test_pred = model.predict(test_data.text.values)\n",
        "pred =  pd.DataFrame(test_pred, columns=['preds'])\n",
        "pred.plot.hist()\n",
        "\n",
        "\n",
        "submission = pd.read_csv(\"https://raw.githubusercontent.com/Konerusudhir/kaggle_datasets/master/nlp/disaster/sample_submission.csv\", error_bad_lines=False)\n",
        "# This for loop its for round predictions\n",
        "submission['target'] = findClasses(test_pred)\n",
        "print(submission.head())\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.82      0.88      0.85       852\n",
            "    Not Real       0.83      0.75      0.79       671\n",
            "\n",
            "    accuracy                           0.82      1523\n",
            "   macro avg       0.82      0.81      0.82      1523\n",
            "weighted avg       0.82      0.82      0.82      1523\n",
            "\n",
            "   id  target\n",
            "0   0       1\n",
            "1   2       1\n",
            "2   3       1\n",
            "3   9       1\n",
            "4  11       1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD4CAYAAAAdIcpQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pegklLJBmXLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}